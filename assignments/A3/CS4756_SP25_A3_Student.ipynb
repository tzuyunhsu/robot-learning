{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GugcHV0e5IKo"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "Welcome to the programming portion of assignment 3 of CS 4756/5756. In this assignment, you will implement several variations of Actor-Critic algorithms and compare their performances.\n",
        "\n",
        "Concretely, you will:\n",
        "* Implement the Advantage Actor-Critic (A2C) algorithm (Part 1)\n",
        "* Implement the Deep Deterministic Policy Gradient (DDPG) algorithm (Part 2)\n",
        "* CS5756 Only: Implement the Soft-Actor-Critic (SAC) algorithm (Part 3)\n",
        "\n",
        "You will use the FetchReach-v4 environments for this assignment. Refer to the Gymnasium-Robotics website for more details about [this environment](https://robotics.farama.org/envs/fetch/).\n",
        "\n",
        "\n",
        "Please read through the following paragraphs carefully.\n",
        "\n",
        "**Getting Started:** You should complete this assignment on **[Google Colab](https://colab.research.google.com/)**.\n",
        "\n",
        "**Evaluation:**\n",
        "Your code will be tested for correctness and, for certain assignments, speed. For this particular assignment, performance results will not be harshly graded (although we provide approximate expected reward numbers, you are not expected to replicate them exactly). Please remember that all assignments should be completed individually.\n",
        "\n",
        "**Academic Integrity:** We will be checking your code against other submissions in the class for logical redundancy. If you copy someone elseâ€™s code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please donâ€™t try. We trust you all to submit your own work only; please donâ€™t let us down. If you do, we will pursue the strongest consequences available to us.\n",
        "\n",
        "**Getting Help:** The [Resources](https://www.cs.cornell.edu/courses/cs4756/2025sp/#resources) section on the course website is your friend! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. If you need a refresher for PyTorch, please see this [60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)! For Numpy, please see the quickstart [here](https://numpy.org/doc/stable/user/quickstart.html) and full API [here](https://numpy.org/doc/stable/reference/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d52C6fHT9eR1"
      },
      "source": [
        "# **Setup**\n",
        "\n",
        "**Please run the cells below to install the necessary packages**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pEa_6lD3-OHI"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "USING_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if USING_COLAB:\n",
        "  !apt-get -qq update\n",
        "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "else:\n",
        "  !pip install torch torchvision torchaudio\n",
        "  !pip install numpy\n",
        "  !pip install tqdm\n",
        "  !pip install opencv-python\n",
        "\n",
        "!pip install matplotlib\n",
        "!pip install -U mediapy\n",
        "!pip install -U renderlab\n",
        "!pip install -U \"imageio<3.0\"\n",
        "!pip install stable-baselines3[extra]\n",
        "!git clone https://github.com/Farama-Foundation/Gymnasium-Robotics.git\n",
        "!pip install -e Gymnasium-Robotics\n",
        "sys.path.append('/content/Gymnasium-Robotics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5LxCSgkZJgL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Mujoco GLEW Setup\n",
        "try:\n",
        "  if _mujoco_run_once:\n",
        "    pass\n",
        "except NameError:\n",
        "  _mujoco_run_once = False\n",
        "if not _mujoco_run_once:\n",
        "  try:\n",
        "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  except KeyError:\n",
        "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  # presetup so we don't see output on first env initialization\n",
        "  _mujoco_run_once = True\n",
        "  if USING_COLAB:\n",
        "    NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "    if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "      with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "          f.write(\"\"\"{\n",
        "          \"file_format_version\" : \"1.0.0\",\n",
        "          \"ICD\" : {\n",
        "              \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "          }\n",
        "      }\n",
        "      \"\"\")\n",
        "  # Set environment variable to support EGL (off-screen) rendering\n",
        "  %env MUJOCO_GL=egl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhHPfeX8SdVy"
      },
      "source": [
        "# Import packages and initial seeding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7QH1iD8ZDeZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as D\n",
        "from stable_baselines3.common.buffers import ReplayBuffer\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import gymnasium_robotics\n",
        "import gymnasium.wrappers as wrappers\n",
        "import random\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QvsVcrNZFem"
      },
      "outputs": [],
      "source": [
        "seed = 695\n",
        "\n",
        "# Setting the seed to ensure reproducability\n",
        "def reseed(seed : int):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "reseed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7NJsMDOawJd"
      },
      "outputs": [],
      "source": [
        "# In this block we define wrappers necessary to simplify the environment MDP\n",
        "def wrap_reach_fixed_goal(env):\n",
        "  g = np.array([1.486, 0.73, 0.681], dtype=np.float32)\n",
        "  env.unwrapped._sample_goal = lambda: g\n",
        "  return env\n",
        "\n",
        "class FetchRewardWrapper(gym.Wrapper):\n",
        "  def reset(self, *args, **kwargs):\n",
        "    obs, info = self.env.reset(*args, **kwargs)\n",
        "    self.prev_dist = np.linalg.norm(obs['achieved_goal'] - obs['desired_goal'])\n",
        "    return obs, info\n",
        "\n",
        "  def step(self, action):\n",
        "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "    achieved_goal = obs['achieved_goal']\n",
        "    desired_goal = obs['desired_goal']\n",
        "    current_dist = np.linalg.norm(achieved_goal - desired_goal)\n",
        "    reward = (self.prev_dist - current_dist) * 10\n",
        "    self.prev_dist = current_dist\n",
        "    terminated = terminated or info['is_success']\n",
        "    return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq62xgt0_q_3"
      },
      "source": [
        "# Visualize Helper Function\n",
        "\n",
        "Below, we provide the helper function `visualize` for your use. This function will create a visualization of the environment passed in the parameter `env`. If you are using Colab, calling this function will render the visualization within the notebook. If you are using your local machine, this function will instead save a video of the visualization to your current directory (rendering videos in Jupyter Notebooks is not widely supported outside of Colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlAWmVPoBJz4"
      },
      "outputs": [],
      "source": [
        "def visualize(env : gym.Env, algorithm=None, video_name=\"test\"):\n",
        "    \"\"\"Visualize a policy network for a given algorithm on a single episode\n",
        "\n",
        "        Args:\n",
        "            algorithm (PolicyGradient): Algorithm whose policy network will be rolled out for the episode. If\n",
        "            no algorithm is passed in, a random policy will be visualized.\n",
        "            video_name (str): Name for the mp4 file of the episode that will be saved (omit .mp4). Only used\n",
        "            when running on local machine.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_action(obs):\n",
        "        if not algorithm:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            return algorithm.compute_action(obs)\n",
        "\n",
        "    if USING_COLAB:\n",
        "        import renderlab as rl\n",
        "\n",
        "        directory = './video'\n",
        "        env = rl.RenderFrame(env, \"output/\")\n",
        "        obs, info = env.reset()\n",
        "        for i in range(500):\n",
        "            action = get_action(obs)\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        env.play()\n",
        "    else:\n",
        "        import cv2\n",
        "\n",
        "        video = cv2.VideoWriter(f\"{video_name}.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 24, (600,400))\n",
        "        obs = env.reset()\n",
        "        for i in range(500):\n",
        "            action = get_action(obs)\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "            im = env.render(mode='rgb_array')\n",
        "            im = im[:,:,::-1]\n",
        "\n",
        "            video.write(im)\n",
        "\n",
        "        video.release()\n",
        "        env.close()\n",
        "        print(f\"Video saved as {video_name}.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd2K7QfzIdQt"
      },
      "source": [
        "### `PolicyNet` class\n",
        "\n",
        "Paste below your implementation of the PolicyNet class from A2. Recall:\n",
        "\n",
        "The `PolicyNet` class should define a neural network that takes in a state and outputs a probability distribution over the action space. The network should have the following architecture:\n",
        "\n",
        "- Input layer: a fully-connected layer with `state_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Output layer: a fully-connected layer with `hidden_dim` input nodes and `action_dim * 2` output nodes (first half for $\\mu(s_t)$, second half for $\\log(\\sigma(s_t))$).\n",
        "\n",
        "\n",
        "Finally, after the output layer, return a `torch.distributions.normal.Normal` object with the proper $\\mu(s_t)$ and $\\sigma(s_t)$.\n",
        "\n",
        "[Documentation on `torch.distributions.normal.Normal`](https://pytorch.org/docs/stable/distributions.html#normal)\n",
        "\n",
        "> You should use the `nn` module of PyTorch to define this network, and use `D.Normal` to construct the final return distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AjrGO7jISGX"
      },
      "outputs": [],
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int):\n",
        "        \"\"\"Policy network architecture for policy gradient algorithms.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): Dimension of the state space.\n",
        "            action_dim (int): Dimension of the action space.\n",
        "            hidden_dim (int): Dimension of the hidden layers.\n",
        "        \"\"\"\n",
        "        super(PolicyNet, self).__init__()\n",
        "        # TODO: Implement the policy network for the REINFORCE algorithm here\n",
        "\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> D.Normal:\n",
        "        \"\"\"Forward pass of the policy network.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): State of the environment. Shape (N, state_dim)\n",
        "        Returns:\n",
        "            action_dist (D.Normal): Normal distribution representing \\pi(a_t | s_t)\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass of the policy network here.\n",
        "        return NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u322aMAspfpr"
      },
      "source": [
        "## **Notes about Fetch Reach Environment**\n",
        "The environment uses a Fetch Robot, which is a 7-DoF Mobile Manipulator.\n",
        "\n",
        "The task is a *goal-reaching task*:\n",
        "The observation space contains '`observation`' which includes the state of the robot in the environment, and '`desired_goal`' which specifies the xyz coordinate that the robot's gripper aims to reach.\n",
        "\n",
        "See https://robotics.farama.org/envs/fetch/reach/ for more details.\n",
        "\n",
        "\n",
        "If the goal is reached, `info['is_success']` will be set to 1, and this is an indication that we should terminate the rollout.\n",
        "\n",
        "\n",
        "The reward is -1 per timestep spent in the environment without completing the task, with 50 steps being the limit (so -50 is the worst episode return).\n",
        "\n",
        "> Note: For this assignment, we've modified the environment that it only has a fixed goal to reach, and has better reward shaping, since vanilla REINFORCE will produce very noisy gradients for complex markov chains.\n",
        "\n",
        "**Run the cell below to create and visualize the environment:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoqC_reVB9hM"
      },
      "outputs": [],
      "source": [
        "# Let's initialize the environment first\n",
        "reseed(seed)\n",
        "env = gym.make(\"FetchReach-v4\", render_mode=\"rgb_array\")\n",
        "env = wrap_reach_fixed_goal(env)\n",
        "env = FetchRewardWrapper(env)\n",
        "env = wrappers.FilterObservation(env, [\"desired_goal\", \"observation\"])\n",
        "env = wrappers.FlattenObservation(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi_4XxI__m4u"
      },
      "outputs": [],
      "source": [
        "visualize(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UeVdhih_hf-"
      },
      "source": [
        "\n",
        "### `PolicyGradient` class\n",
        "\n",
        "Here, we implement a basic version of the `PolicyGradient` class from A2.\n",
        "\n",
        "Recall the following methods (that you will be implementing for your algorithms).\n",
        "\n",
        "- `compute_action(self, state)`: Method that selects an action based on the policy network.\n",
        "\n",
        "- `compute_loss(self, episode, gamma)`: Method that computes the loss for a given episode.\n",
        "\n",
        "- `update_policy(self, episodes, optimizer, gamma)`: Method that updates the policy network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfgB5aJq_fPx"
      },
      "outputs": [],
      "source": [
        "# No need to implement anything in this cell, just run it.\n",
        "\n",
        "class PolicyGradient:\n",
        "    def __init__(\n",
        "      self,\n",
        "      policy_net : PolicyNet\n",
        "    ):\n",
        "      \"\"\"Default class for a policy gradient algorithm.\n",
        "\n",
        "      Args:\n",
        "          policy_net (PolicyNet): Policy network\n",
        "      \"\"\"\n",
        "      self.policy_net = policy_net\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "      return next(iter(self.policy_net.parameters())).device\n",
        "\n",
        "    def to(self, device : str | torch.device):\n",
        "      self.policy_net.to(device)\n",
        "      return self\n",
        "\n",
        "    def compute_action(self, state : np.ndarray) -> np.ndarray:\n",
        "        raise NotImplementedError(\"Make sure you implemented compute_action()!\")\n",
        "\n",
        "    def compute_loss(\n",
        "      self,\n",
        "      episode : list[tuple[\n",
        "          np.ndarray, np.ndarray, float\n",
        "      ]],\n",
        "      gamma : float\n",
        "    ) -> torch.Tensor:\n",
        "      raise NotImplementedError(\"Make sure you implemented compute_loss()!\")\n",
        "\n",
        "    def update_policy(self, episodes, optimizer, gamma):\n",
        "      raise NotImplementedError(\"Make sure you implemented update_policy()!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHNQqc7_GK3Y"
      },
      "outputs": [],
      "source": [
        "def collect_episode(env : gym.Env, policy : PolicyGradient | None = None, seed : int | None = None):\n",
        "    \"\"\"\n",
        "    Run an episode of the environment and return the episode\n",
        "\n",
        "    Returns:\n",
        "        episode (list): List of tuples (state, action, reward)\n",
        "    \"\"\"\n",
        "    state, info = env.reset(seed=seed)\n",
        "    episode = []\n",
        "    done = False\n",
        "    while not done:\n",
        "      if policy is not None:\n",
        "        action = policy.compute_action(state)\n",
        "      else:\n",
        "        action = env.action_space.sample()\n",
        "      next_state, reward, terminated, truncated, info = env.step(action)\n",
        "      done = terminated or truncated or info['is_success']\n",
        "      episode.append((state, action, reward, next_state, done, info))\n",
        "      state = next_state\n",
        "    return episode\n",
        "\n",
        "def evaluate(env : gym.Env, policy : PolicyGradient | None = None, num_episodes = 100):\n",
        "    \"\"\"Evaluate the policy network by running multiple episodes.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): Environment to evaluate the policy network on\n",
        "        policy (PolicyGradient): Policy network to evaluate\n",
        "        num_episodes (int): Number of episodes to run\n",
        "\n",
        "    Returns:\n",
        "        average_reward (float): Average total reward per episode\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "    for i in range(num_episodes):\n",
        "      episode = collect_episode(env, policy)\n",
        "      episode_rewards.append(sum([reward for _, _, reward, _, _, _ in episode]))\n",
        "    return np.mean(episode_rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H9zQBy_zg0X"
      },
      "source": [
        "# Part 1: Advantage Actor-Critic\n",
        "\n",
        "In this part of the assignment, we will implement a version of the Advantage Actor-Critic (A2C) algorithm. This algorithm is an extension to the REINFORCE algorithm, and it combines both value-based and policy-based methods. **See MACRL 11.8 for more information on Actor-Critic methods.**\n",
        "\n",
        "## Overview\n",
        "\n",
        "The A2C algorithm reduces the variance of the model by subtracting a baseline from the sum of rewards. This modifies our original objective function as follows:\n",
        "\n",
        "$$ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[r(\\tau) - b] $$\n",
        "\n",
        "where $b$ is the baseline.\n",
        "\n",
        "In A2C, there is both a policy network $\\pi_{\\theta}$ (actor network) and a value network $V_Ï•$ (critic network). The policy network is used to select actions, while the value network is used to estimate the value of a state. The value network will be used to compute the baseline $b$ in A2C.\n",
        "\n",
        "Specifically, using $V_{\\phi}(s_t)$ as the baseline, and using (discounted) reward-to-go in place of total reward (as done in part 2), A2C approximates $ \\nabla_\\theta J(\\theta)$ as follows:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N}  \\sum_{t=0}^{T-1} \\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it}) \\bigl(R_{it} - V_Ï• (s_{it}) \\bigl) $$\n",
        "\n",
        "This algorithm is called **Advantage** Actor-Critic because the quantity $R_t - V_Ï• (s_{t})$ is an approximation of what is known as the *advantage function* $A^{\\pi_{\\theta}}$:\n",
        "\n",
        "$$A^{\\pi_{\\theta}}(s_t,a_t) = Q^{\\pi_{\\theta}}(s_t,a_t) - V^{\\pi_{\\theta}}(s_t) \\approx R_t - V_Ï• (s_t)$$\n",
        "\n",
        "where $Q^{\\pi_{\\theta}}(s_t,a_t)$ and $V^{\\pi_{\\theta}}(s_t)$ are the actual exact Q and value functions for ${\\pi_{\\theta}}$.\n",
        "\n",
        "Defining $\\tilde{A_t} = R_t - V_Ï• (s_t)$ to be the approximate advantage function, we have:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N}  \\sum_{t=0}^{T-1} \\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it}) \\tilde{A_{it}}$$\n",
        "\n",
        "For A2C, we have two loss functions: one for the policy network and one for the value network. The policy loss will be defined similarly to the loss in reward-to-go REINFORCE, replacing $R_t$ with $\\tilde{A_t}$ and additionally taking the average loss within each episode rather than using total loss (as is commonly done in practice):\n",
        "\n",
        "$$ L_{\\text{policy}} = - \\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{T}\\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_{it}|s_{it}) \\tilde{A_{it}} $$\n",
        "\n",
        "The value loss will be the average (across episodes) mean squared error between the reward to go and the estimated value of the state. But note that we already defined the difference between reward to go and estimated value as the approximate advantage function. So we have:\n",
        "\n",
        "$$ L_{\\text{value}} = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{T}\\sum_{t=0}^{T} (R_{it} - V_{\\phi}(s_{it}))^2 = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{T}\\sum_{t=0}^{T} \\tilde{A_{it}}^2$$\n",
        "\n",
        "## Instructions\n",
        "\n",
        "You will need to implement the following:\n",
        "\n",
        "1. `ValueNet` class - This class will define the value network used in the A2C algorithm.\n",
        "\n",
        "2. `ActorCriticPolicyGradient` class - This class will define the A2C algorithm. It will be a modified version of the `PolicyGradient` class that includes the value network and the A2C loss functions.\n",
        "\n",
        "Follow the instructions below to implement each of these components.\n",
        "\n",
        "### `ValueNet` class\n",
        "\n",
        "The `ValueNet` class should define a neural network that takes in a state and outputs an estimate of the value of that state. The network should have the following architecture:\n",
        "\n",
        "- Input layer: a fully-connected layer with `state_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Output layer: a fully-connected layer with `hidden_dim` input nodes and 1 output node.\n",
        "\n",
        "You should use the `nn` module of PyTorch to define this network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgFvW2xBBa9G"
      },
      "outputs": [],
      "source": [
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, state_dim: int, hidden_dim: int):\n",
        "        \"\"\"Value network for the Advantage Actor-Critic algorithm.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): Dimension of the state space.\n",
        "            hidden_dim (int): Dimension of the hidden layers.\n",
        "        \"\"\"\n",
        "        #TODO: Implement the ValueNet architecture.\n",
        "        super(ValueNet, self).__init__()\n",
        "\n",
        "    def forward(self, state: torch.Tensor):\n",
        "        \"\"\"Forward pass of the value network.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): State of the environment.\n",
        "\n",
        "        Returns:\n",
        "            x (torch.Tensor): Estimated value of the state.\n",
        "        \"\"\"\n",
        "        #TODO: Implement the ValueNet forward pass.\n",
        "        return NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylPbU9DQCWDP"
      },
      "source": [
        "### `ActorCriticPolicyGradient` class\n",
        "\n",
        "The `ActorCriticPolicyGradient` class should define the A2C algorithm.\n",
        "\n",
        "The `ActorCriticPolicyGradient` class should override the following methods from `PolicyGradient`:\n",
        "\n",
        "- `__init__(self, env, policy_net: PolicyNet, value_net: ValueNet)`: Constructor method that initializes the environment, policy network, and value network.\n",
        "\n",
        "- `compute_action(self, state)`: Method that selects an action based on the policy network.\n",
        "\n",
        "- `compute_loss(self, episode, gamma)`: Method that computes the two loss functions for the A2C algorithm.\n",
        "\n",
        "- `update_policy(self, episodes, optimizer, value_optimizer, gamma)`: Method that updates the policy network and value network using a batch of episodes.\n",
        "\n",
        "- `train(self, num_episodes, batch_size, gamma, lr)`: Method that trains the policy network and value network using the A2C algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9lLpaSjB_5R"
      },
      "outputs": [],
      "source": [
        "class ActorCriticPolicyGradient(PolicyGradient):\n",
        "    def __init__(self, policy_net, value_net):\n",
        "        \"\"\"A2C algorithm.\n",
        "\n",
        "        Args:\n",
        "            policy_net (PolicyNet): Policy network\n",
        "            value_net (ValueNet): Value network\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(policy_net)\n",
        "        self.policy_net = policy_net.to(self.device)\n",
        "        self.value_net = value_net.to(self.device)\n",
        "\n",
        "    def compute_action(self, state : np.ndarray) -> np.ndarray:\n",
        "      \"\"\"Select an action based on the policy network\n",
        "\n",
        "      Args:\n",
        "          state (np.ndarray): State of the environment\n",
        "\n",
        "      Returns:\n",
        "          action (np.ndarray): Action to take\n",
        "      \"\"\"\n",
        "      # TODO: Implement the action selection here based on the policy network output probabilities\n",
        "      # You also need to convert between numpy arrays and torch tensors\n",
        "      # HINT: Use `torch.no_grad`\n",
        "\n",
        "    def compute_loss(self, episode, gamma):\n",
        "        \"\"\"Compute the loss function for the A2C algorithm\n",
        "\n",
        "        Args:\n",
        "            episode (list): List of tuples (state, action, reward)\n",
        "\n",
        "        Returns:\n",
        "            policy_loss (torch.Tensor): Value of policy loss function\n",
        "            value_loss (torch.Tensor): Value of value loss function\n",
        "        \"\"\"\n",
        "        #TODO: Implement the A2C loss function.\n",
        "\n",
        "        # TODO1: Extract states, actions and rewards from the episode.\n",
        "\n",
        "        # TODO2: Compute the reward-to-go.\n",
        "\n",
        "        # TODO3: Compute the advantages and log probabilities.\n",
        "\n",
        "        # TODO4: Compute the policy and value function loss.\n",
        "\n",
        "        return NotImplementedError()\n",
        "\n",
        "    def update_policy(self, episodes, optimizer, value_optimizer, gamma):\n",
        "        \"\"\"Update the policy network and value network using the batch of episodes\n",
        "\n",
        "        Args:\n",
        "            episodes (list): List of episodes\n",
        "            optimizer (torch.optim): Optimizer for policy network\n",
        "            value_optimizer (torch.optim): Optimizer for value network\n",
        "            gamma (float): Discount factor\n",
        "        \"\"\"\n",
        "        # TODO: Compute the loss function for each episode\n",
        "\n",
        "        return NotImplementedError()\n",
        "\n",
        "    def train(self, env: gym.Env, num_iterations: int, batch_size: int, gamma: float, lr: float) -> None:\n",
        "      \"\"\"Train the policy and value networks using the A2C algorithm.\n",
        "\n",
        "      Args:\n",
        "          num_iterations (int): Number of training iterations.\n",
        "          batch_size (int): Number of episodes per batch.\n",
        "          gamma (float): Discount factor.\n",
        "          lr (float): Learning rate.\n",
        "      \"\"\"\n",
        "\n",
        "      # TODO: Implement the training loop for A2C. Very similar to all of your past training loops.\n",
        "      return NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have implemented A2C, it is time to train and evaluate a policy. Below, we provide training hyperparameters which you should use in your experiments. See the writeup for additional instructions on what metrics and figures you need to include in your submission.\n",
        "\n",
        "* Expected Training Time (Colab CPU): 15 minutes\n",
        "* Expected Episodic Reward: > 1.3"
      ],
      "metadata": {
        "id": "xQMcjhGjZG-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pre-training average return\", evaluate(env))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcvh0NCJZeP4",
        "outputId": "526432b9-3973-49e6-b9cd-3dd80032d1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-training average return -0.6835955719662004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uWGZwUMtG-UU"
      },
      "outputs": [],
      "source": [
        "# Feel free to use the space below to run experiments and create plots used in your writeup.\n",
        "reseed(seed)\n",
        "env.reset(seed=seed)\n",
        "env.action_space.seed(seed)\n",
        "env.observation_space.seed(seed)\n",
        "\n",
        "policy_net_a2c = PolicyNet(env.observation_space.shape[-1], env.action_space.shape[-1], 128)\n",
        "value_net = ValueNet(env.observation_space.shape[-1], 128)\n",
        "a2c = ActorCriticPolicyGradient(policy_net_a2c, value_net)\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "  device = \"mps\"\n",
        "\n",
        "\n",
        "a2c.to(device)\n",
        "losses = a2c.train(env, num_iterations=300, batch_size=16, gamma=0.99, lr=7e-4)\n",
        "\n",
        "print(\"Average reward:\", evaluate(env, a2c, 10))\n",
        "\n",
        "visualize(env, algorithm=a2c, video_name=\"a2c\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Please submit two separate plots in your writeup, of the value and policy loss across training. As well, note your average reward."
      ],
      "metadata": {
        "id": "CTRPGeFqaM5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your plotting code here, if you like."
      ],
      "metadata": {
        "id": "ECqX1PfSadK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2: Deep Deterministic Policy Gradient (DDPG)\n",
        "\n",
        "In this part of the assignment, we will implement a version of the Deep Deterministic Policy Gradient (DDPG) algorithm. Deep Deterministic Policy Gradient (DDPG) is an off-policy, actor-critic algorithm designed for continuous action spaces. It extends the ideas of Q-learning to the continuous domain by combining a deterministic policy (actor) with a Q-function approximator (critic).\n",
        "\n",
        "\n",
        "DDPG learns a deterministic policy:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mu_{\\theta}(s) \\approx a\n",
        "\\end{equation}\n",
        "\n",
        "where: $\\mu_{\\theta}(s)$ is the policy (actor) network parameterized by $\\theta$,\n",
        "$s$ is the current state,\n",
        "$a$ is the deterministic action output.\n",
        "\n",
        "This part requires three steps:\n",
        "1. Implement `CriticNetwork()`\n",
        "2. Implement `DeterministicActor()`\n",
        "3. Implement `DDPGAgent()`\n",
        "\n",
        "Below, we walk you through the steps."
      ],
      "metadata": {
        "id": "MQCPMqNWanQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `CriticNetwork` class\n",
        "\n",
        "The `CriticNetwork` class defines the \"q-function\" we have been discussing in classs; it should be a neural network that takes in a (state, action) pair, and outputs an estimate of the value of rolling out that (state, action) pair in the environment. The network should have the following architecture:\n",
        "\n",
        "- Input layer: a fully-connected layer with `state_dim + action_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Intermediate layer: a fully connected `hidden_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Output layer: a fully-connected layer with `hidden_dim` input nodes and 1 output node.\n",
        "\n",
        "You should use the `nn` module of PyTorch to define this network."
      ],
      "metadata": {
        "id": "q10bM1jLbS3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        #TODO: Implement the CriticNetwork architecture.\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        #TODO: Implement the CriticNetwork forward pass.\n",
        "        return NotImplementedError()"
      ],
      "metadata": {
        "id": "99BMGI7gOnAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `DeterministicActor` class\n",
        "\n",
        "The `DeterministicActor` class should define a neural network that takes in a state, and deterministically outputs an action. The network should have the following architecture:\n",
        "\n",
        "- Input layer: a fully-connected layer with `state_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Intermediate layer: a fully-connected layer with `hidden_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Output layer: a fully-connected layer with `hidden_dim` input nodes and `action_dim` output nodes.\n",
        "\n",
        "In DDPG, the actor now directly outputs the action\n",
        "ð‘Ž, rather than a probability distribution over actions (like in stochastic policies). A fully connected neural network with linear output layers can produce arbitrarily small or large values, and so we need some way to constrain our output in the continuous control setting.\n",
        "\n",
        "This is where we use the $\\tanh$ squashing trick, where we apply $\\tanh$ to the neural network's output to constrain it to the range $[-1, 1]$. This allows us to prevent outliers/arbitrarily large actions, but we still need to make sure the action is relevant for our environment.\n",
        "\n",
        "We then want to scale the action to our environment. The actions in our environment are in the range $[\\text{action_low}, \\text{action_high}]$, which are both inputs to our Actor upon initialization. Can you think about how to define an \"action bias\" and \"action scale\" term, to transform our distribution from $[-1, 1]$ to $[\\text{action_low}, \\text{action_high}]$?\n",
        "\n",
        "The final output of the forward method should be an action in the range $[\\text{action_low}, \\text{action_high}]$."
      ],
      "metadata": {
        "id": "omvak5B5cTpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeterministicActor(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_dim, action_high, action_low):\n",
        "        super().__init__()\n",
        "        #TODO: Implement the DeterministicActor architecture.\n",
        "\n",
        "    def forward(self, x):\n",
        "        #TODO: Implement the DeterministicActor forward pass.\n",
        "        # TODO1: passes the input through the two layers, each followed by a ReLU activation\n",
        "        # TODO2: scales the raw output within [-1,1]\n",
        "        # TODO3: rescale the raw outputs to match the environment action space.\n",
        "        return NotImplementedError()"
      ],
      "metadata": {
        "id": "YkfdtVBcdAbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `DDPGAgent` class\n",
        "\n",
        "The `DDPGAgent` class should define the DDPG algorithm. Recall that DDPG is an **off-policy algorithm**, meaning that we don't have to learn from temporally consistent episodes; instead, we sample batches of **transitions**, which consist of (state, next_state, action, reward, done, info), and then backpropagate the loss of our actor/critic to update the networks.\n",
        "\n",
        "We do this by adding every transition we experience in the environment to a \"replay buffer\". This is the concept of \"experience replay\", and it is a widely used technique across reinforcement/robot learning algorithms ([read here if you want to learn more](https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits)). In this assignment, we use the Stable-Baselines3 replay buffer.\n",
        "\n",
        "The basic structure of the `DDPGAgent` consists of:\n",
        "- One `CriticNetwork()` and one target `CriticNetwork()`\n",
        "- One `DeterministicActor()` and one target `DeterministicActor()`.\n",
        "\n",
        "We use target networks to help stabilize training and reduce overestimation bias.\n",
        "\n",
        "The formulation of DDPG is as follows:\n",
        "\n",
        "Suppose we sampled a batch of transitions of form $(s, s', a, r, d, i)$:\n",
        "\n",
        "We then want to compute the **target q-values** like so:\n",
        "\n",
        "$$y(r, s', d) = r + \\gamma (1-d)Q_{targ}(s', \\mu_{\\theta_{targ}}(s'))$$\n",
        "\n",
        "The **critic** is then updated by one step of gradient descent like:\n",
        "\n",
        "$$âˆ‡\\frac{1}{|B|}âˆ‘_{(s, s', a, r, d) \\in B}(Q(s, a)-y(r, s', d))^2$$\n",
        "\n",
        "Next, we simply use queries of the critic to provide loss for the actor - intuitively, we want the actor to maximize the Q-function, and so we perform gradient **ascent** on the quantity:\n",
        "\n",
        "$$âˆ‡\\frac{1}{|B|}âˆ‘_{s \\in B}Q(s, \\mu_\\theta(s))$$\n",
        "\n",
        "Finally, we update the parameters of the target network according to a rate $\\tau$; denoting the parameters of the critic by $\\phi$ and the actor by $\\theta$ gives the update step as:\n",
        "\n",
        "$$\\phi_{target} = (1-\\tau)\\phi_{target} + \\tau\\phi$$\n",
        "\n",
        "$$\\theta_{target} = (1-\\tau)\\theta_{target} + \\tau\\theta$$\n",
        "\n",
        "We want you to implement the following methods:\n",
        "\n",
        "- `train(self, num_iterations, batch_size)`: This is the method that trains the actor and critic in the DDPG algorithm. It should follow this basic format:\n",
        "  - For each iteration in num_iterations, you should step through the environment, and add the transition to your replay buffer. Once you have accumulated batch_size transitions, call the `update_policy()` method every step.\n",
        "  - Print out some useful training information, and track your losses.\n",
        "  - **IMPORTANT**: When stepping through the environment, don't forget to set obs = next_obs when you are finished with the step!\n",
        "\n",
        "- `compute_action(self, state)`: Since DDPG follows deterministic policy, we add Gaussian noise for exploration:\n",
        "\\begin{equation}\n",
        "a = \\mu_\\theta(s) + \\mathcal{N}\n",
        "\\end{equation}\n",
        "where $\\mathcal{N}$ is the Gaussian noise with mean of 0 and standard deviation of 0.1 (default).\n",
        "\n",
        "- `compute_critic_loss(self, data)`: Method that computes the loss function for your critic network.\n",
        "\n",
        "- `compute_actor_loss(self, data)`: Method that computes the loss function for your actor network. We apply L2 regularization with `policy_pre_activation_weight` to the actor loss function to penalize large action values. The regularization is implemented for you, you will only need to compute the Q values for policy actions and compute the raw actor loss.\n",
        "\n",
        "\n",
        "- `update_target_net(self)`: Method that performs the weighted target net update.\n",
        "\n",
        "- `update_policy(self, batch_size)`: Method that updates the actor and critic using batch of transitions. It should follow this basic format:\n",
        "  - Sample `batch_size` transitions from your replay buffer.\n",
        "  - Call `compute_critic_loss(self, data)`, and update the critic network.\n",
        "  - Call `compute_actor_loss(self, data)`, and update the actor network.\n",
        "  - Perform the target network update.\n",
        "\n",
        "\n",
        "**Important** Please do not deviate from the `DDPGAgent()` format/method structure, it makes it much easier for us to help you in office hours!"
      ],
      "metadata": {
        "id": "_fDEaj9_fmyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPGAgent():\n",
        "    def __init__(self, env, gamma, tau, actor_lr, critic_lr, network_hidden_dim):\n",
        "\n",
        "        # You should not have to change anything in the init() method, we provide this for your convenience.\n",
        "\n",
        "        self.env = env\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        obs_dim = np.prod(self.env.observation_space.shape)\n",
        "        act_dim = np.prod(self.env.action_space.shape)\n",
        "\n",
        "        self.actor = DeterministicActor(\n",
        "            obs_dim, act_dim, network_hidden_dim,\n",
        "            torch.tensor(self.env.action_space.high, dtype=torch.float32, device=self.device),\n",
        "            torch.tensor(self.env.action_space.low, dtype=torch.float32, device=self.device)\n",
        "        ).to(self.device)\n",
        "        self.actor_target = DeterministicActor(\n",
        "            obs_dim, act_dim, network_hidden_dim,\n",
        "            torch.tensor(self.env.action_space.high, dtype=torch.float32, device=self.device),\n",
        "            torch.tensor(self.env.action_space.low, dtype=torch.float32, device=self.device)\n",
        "        ).to(self.device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "\n",
        "        self.critic = CriticNetwork(obs_dim, act_dim, network_hidden_dim).to(self.device)\n",
        "        self.critic_target = CriticNetwork(obs_dim, act_dim, network_hidden_dim).to(self.device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(int(1e6), self.env.observation_space, self.env.action_space, self.device, handle_timeout_termination=False)\n",
        "\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "\n",
        "        self.tau = tau\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.policy_pre_activation_weight = 0.1\n",
        "        self.min_q_value = -np.inf\n",
        "        self.max_q_value = np.inf\n",
        "\n",
        "\n",
        "\n",
        "    def compute_action(self, obs, noise_std=0.1):\n",
        "\n",
        "      # TODO1: compute raw action\n",
        "      # TODO2: add Gaussian noise with mean 0 and provided noise_std\n",
        "      # TODO3: return clipped action within environment's action space\n",
        "      return NotImplementedError()\n",
        "\n",
        "    def train(self, num_iterations, batch_size):\n",
        "      # TODO: Implement DDPG training loop.\n",
        "      # TODO1: Set up stepping through the environment, and adding transitions to replay buffer.\n",
        "      # TODO2: Update the policy with batch size.\n",
        "      # TODO3: Print out useful information and losses.\n",
        "\n",
        "\n",
        "    def compute_critic_loss(self, data):\n",
        "      # TODO: Compute critic loss function.\n",
        "      # TODO1: compute target Q-value\n",
        "      # TODO2: compute predicted Q-value\n",
        "      # TODO3: return Critic Loss (MSE loss)\n",
        "      return NotImplementedError()\n",
        "\n",
        "\n",
        "    def compute_actor_loss(self, data):\n",
        "      #this is implemented for you in case where pre-activation is needed\n",
        "      if self.policy_pre_activation_weight > 0 and hasattr(self.actor, \"forward_with_preactivations\"):\n",
        "          policy_actions, pre_tanh_value = self.actor.forward_with_preactivations(data.observations)\n",
        "      else:\n",
        "          #TODO: step 1: compute action\n",
        "\n",
        "      # TODO: Step 2: compute Q-value\n",
        "\n",
        "      # TODO: Step 3: compute raw policy loss (maximize Q-values means minimize...)\n",
        "\n",
        "      #If pre-activation regularization is enabled, compute the penalty\n",
        "      if self.policy_pre_activation_weight > 0 and hasattr(self.actor, \"forward_with_preactivations\"):\n",
        "          pre_activation_policy_loss = ((pre_tanh_value**2).sum(dim=1).mean())\n",
        "          policy_loss = policy_loss + pre_activation_policy_loss * self.policy_pre_activation_weight\n",
        "\n",
        "      return policy_loss\n",
        "\n",
        "    def update_target_net(self):\n",
        "      # TODO: Implement target net updating.\n",
        "\n",
        "    def update(self, batch_size):\n",
        "      # TODO: Implement the update step for the networks."
      ],
      "metadata": {
        "id": "FaMBkkMvfhOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have implemented DDPG, it is time to train and evaluate a policy. Below, we provide training hyperparameters which you should use in your experiments. See the writeup for additional instructions on what metrics and figures you need to include in your submission.\n",
        "\n",
        "* Expected Training Time (Colab CPU): < 5 minutes\n",
        "* Expected Episodic Reward: ~ 1.3"
      ],
      "metadata": {
        "id": "XSePGIvVHJLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reseed(seed)\n",
        "env.reset(seed=seed)\n",
        "env.action_space.seed(seed)\n",
        "env.observation_space.seed(seed)\n",
        "\n",
        "ddpg_agent = DDPGAgent(\n",
        "    env=env,\n",
        "    gamma=0.99,\n",
        "    tau=0.003,\n",
        "    actor_lr=5e-6,\n",
        "    critic_lr=1e-5,\n",
        "    network_hidden_dim=256\n",
        ")\n",
        "\n",
        "num_iterations = 3000\n",
        "batch_size = 256\n",
        "ddpg_agent.train(num_iterations=num_iterations, batch_size=batch_size)\n",
        "\n",
        "average_reward = evaluate(env, ddpg_agent, 10)\n",
        "print(f\"Average reward after training: {average_reward:.2f}\")\n",
        "visualize(env, algorithm=ddpg_agent, video_name=\"ddpg\")"
      ],
      "metadata": {
        "id": "Mn5ZRrssCmnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Please submit two separate plots in your writeup, of the value and policy loss across training. As well, note your average reward."
      ],
      "metadata": {
        "id": "gpem5uzmCqod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to use this space for plotting."
      ],
      "metadata": {
        "id": "P-gjrLlYCrX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#(CS5756 ONLY) Part 3: Soft Actor Critic (SAC)\n",
        "\n",
        "In this part of the assignment, we will implement a version of the Soft Actor Critic (SAC) algorithm. SAC is an off-policy, actor-critic algorithm designed for continuous action spaces. Unlike DDPG, SAC learns a **stochastic** policy and incorporates an **entropy** term in the objective function to encourage exploration. This helps SAC achieve better stability and sample efficiency.\n",
        "\n",
        "This part requires three steps:\n",
        "1. Implement `StochasticActor()`\n",
        "3. Implement `SACAgent()`\n",
        "\n",
        "Below, we walk you through the steps."
      ],
      "metadata": {
        "id": "KLMkepPXyOTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `StochasticActor` class\n",
        "\n",
        "The `StochasticActor` class should define a neural network that takes in a state, and outputs an action sampled stochastically from a distribution. The network should have the following architecture:\n",
        "\n",
        "- Input layer: a fully-connected layer with `state_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Intermediate layer: a fully-connected layer with `hidden_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Output layer I: a fully-connected layer with `hidden_dim` input nodes and `action_dim` output nodes for $\\mu(s_t)$.\n",
        "\n",
        "- Output layer II: a fully-connected layer with `hidden_dim` input nodes and `action_dim` output nodes for $\\log(\\sigma(s_t))$).\n",
        "\n",
        "We again use the $\\tanh$ squashing trick from our DDPG implementation, where we apply $\\tanh$ to the neural network's output to constrain it to the range $[-1, 1]$. In the forward function, we do two different things:\n",
        "\n",
        "For output layer I, we simply output the mean.\n",
        "\n",
        "For output layer II, we want to scale the `log_std` distribution from $[-1, 1]$ to $[\\text{LOG_STD_MIN}, \\text{LOG_STD_MAX}]$. Use similar tricks from your DDPG implementation.\n",
        "\n",
        "For `sample_action_distribution(self, x)`, we sample an initial $x_t$ from a `torch.distributions.normal.Normal` object with the proper $\\mu(s_t)$ and $\\sigma(s_t)$. Then, we apply the $\\tanh$ squashing + biasing trick to our action $y_t$ to get our action in the range $[\\text{action_low}, \\text{action_high}]$.\n",
        "\n",
        "To compute the log probability of our action, we take the log probability of our `torch.distributions.normal.Normal` at the sampled $x_t$, and subtract a corrective factor of $\\log(\\text{action_scale} * (1 - y_t^2) + 1e^{-6})$ to account for the change in probability density from our $\\tanh$ squashing trick.\n",
        "\n",
        "The `sample_action_distribution(self, x)` mmethod should then output the scaled output action $y_t$, and the corrected log probability."
      ],
      "metadata": {
        "id": "dPHJ9ftRyfcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_STD_MAX = 2\n",
        "LOG_STD_MIN = -5\n",
        "class StochasticActor(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_dim, action_high, action_low):\n",
        "        super().__init__()\n",
        "        # TODO: Implement the StochasticActor network architecture.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the StochasticActor forward pass.\n",
        "        return NotImplementedError()\n",
        "\n",
        "    def sample_action_distribution(self, x):\n",
        "        # TODO: Implement sampling from the action distribution.\n",
        "        return NotImplementedError()"
      ],
      "metadata": {
        "id": "GZKtSbByOn6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `SACAgent` class\n",
        "\n",
        "The `SACAgent` class should define the SAC algorithm. Recall that SAC is an **off-policy algorithm**, so it trains very similarly to DDPG.\n",
        "\n",
        "The basic structure of the `DDPGAgent` consists of:\n",
        "- TWO `CriticNetwork()` and TWO target `CriticNetwork()`\n",
        "- One `DeterministicActor()`.\n",
        "\n",
        "We use two critic networks in SAC to help reduce overestimation bias.\n",
        "\n",
        "The formulation of SAC is as follows:\n",
        "\n",
        "Suppose we sampled a batch of transitions of form $(s, s', a, r, d, i)$:\n",
        "\n",
        "We then want to compute the **target q-values** like so, denoting $a' \\sim \\mu_\\theta(s')$:\n",
        "\n",
        "$$y(r, s', d) = r + \\gamma (1-d)[\\min(Q_{targ, 1}(s', a'), Q_{targ, 2}(s', a')) - \\alpha\\log\\pi_\\theta(a'|s')]$$\n",
        "\n",
        "where $\\alpha$ is the temperature parameter.\n",
        "\n",
        "The **critic** is then updated by one step of gradient descent like:\n",
        "\n",
        "$$âˆ‡\\frac{1}{|B|}âˆ‘_{(s, s', a, r, d) \\in B}(Q(s, a)-y(r, s', d))^2$$\n",
        "\n",
        "The actor is updated by computing a loss dependent on entropy (which encourages exploration) and the Q-value of the actions taken by the actor (denoting $a = \\mu_\\theta(s)$).\n",
        "\n",
        "$$âˆ‡\\frac{1}{|B|}âˆ‘_{s' \\in B}[\\alpha\\log\\pi_\\theta(a'|s') - \\min(Q_1(s', a'), Q_2(s', a'))]$$\n",
        "\n",
        "Finally, we update the parameters of the target network according to a rate $\\tau$; the update step is:\n",
        "\n",
        "$$\\theta_{target, 1} = (1-\\tau)\\theta_{target, 1} + \\tau\\theta_1$$\n",
        "$$\\theta_{target, 2} = (1-\\tau)\\theta_{target, 2} + \\tau\\theta_2$$\n",
        "\n",
        "We want you to implement the following methods:\n",
        "\n",
        "- `train(self, num_iterations, batch_size)`: This is the method that trains the actor and critic in the SAC algorithm. It should follow this basic format:\n",
        "  - For each iteration in num_iterations, you should step through the environment, and add the transition to your replay buffer. Once you have accumulated batch_size transitions, call the `update_policy()` method every step.\n",
        "  - Print out some useful training information, and track your losses.\n",
        "  - **IMPORTANT**: When stepping through the environment, don't forget to set obs = next_obs when you are finished with the step!\n",
        "\n",
        "- `compute_action(self, state)`: Compute the action using the `sample_action_distribution` method of the Actor.\n",
        "\n",
        "- `compute_critic_loss(self, data)`: Method that computes the loss function for your critic networks.\n",
        "\n",
        "- `compute_actor_loss(self, data)`: Method that computes the loss function for your actor network.\n",
        "\n",
        "- `update_target_net(self)`: Method that performs the weighted target net update.\n",
        "\n",
        "- `update_actor(self, loss)`: Take in a loss computed by `compute_actor_loss(self, data)`, and update the actor network.\n",
        "\n",
        "- `update_actor(self, loss1, loss2)`: Take in losses computed by `compute_critic_loss(self, data)`, and update the critic networks.\n",
        "\n",
        "- `update_policy(self, batch_size)`: Method that updates the actor and critic using batch of transitions. It should follow this basic format:\n",
        "  - Sample `batch_size` transitions from your replay buffer.\n",
        "  - Call `update_critic()`.\n",
        "  - Call `update_actor()`.\n",
        "  - Call `update_target_net()`.\n",
        "\n",
        "\n",
        "**Important** Please do not deviate from the `SACAgent()` format/method structure, it makes it much easier for us to help you in office hours!"
      ],
      "metadata": {
        "id": "n6UKwxN0-L8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SACAgent():\n",
        "    def __init__(self, env, gamma, tau, alpha, actor_lr, critic_lr, network_hidden_dim):\n",
        "\n",
        "        # You should not have to change anything in the init() method, we provide it for your convenience.\n",
        "        self.env = env\n",
        "        self.device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
        "\n",
        "        obs_dim = np.prod(self.env.observation_space.shape)\n",
        "        act_dim = np.prod(self.env.action_space.shape)\n",
        "\n",
        "        self.actor = StochasticActor(obs_dim, act_dim, network_hidden_dim, self.env.action_space.high, self.env.action_space.low).to(self.device)\n",
        "        self.critic1 = CriticNetwork(obs_dim, act_dim, network_hidden_dim).to(self.device)\n",
        "        self.critic2 = CriticNetwork(obs_dim, act_dim, network_hidden_dim).to(self.device)\n",
        "        self.critic1_target = CriticNetwork(obs_dim, act_dim, network_hidden_dim).to(self.device)\n",
        "        self.critic2_target = CriticNetwork(obs_dim, act_dim, network_hidden_dim).to(self.device)\n",
        "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
        "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(int(1e6), self.env.observation_space, self.env.action_space, self.device, handle_timeout_termination=False)\n",
        "        self.critic_optimizer = optim.Adam(list(self.critic1.parameters()) + list(self.critic2.parameters()), lr=critic_lr)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "        self.alpha = alpha\n",
        "        self.tau = tau\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def compute_action(self, obs):\n",
        "      # TODO: Implement action computation.\n",
        "      return NotImplementedError()\n",
        "\n",
        "    def train(self, num_iterations, batch_size):\n",
        "      # TODO: Implement training loop.\n",
        "\n",
        "    def compute_critic_loss(self, data):\n",
        "      # TODO: Implement critic loss computation.\n",
        "      return NotImplementedError()\n",
        "\n",
        "    def compute_actor_loss(self, data):\n",
        "      # TODO: Implement actor loss computation.\n",
        "      return NotImplementedError()\n",
        "\n",
        "    def update_critic(self, loss1, loss2):\n",
        "      # TODO: Implement critic update step.\n",
        "\n",
        "    def update_actor(self, loss):\n",
        "      # TODO: Implement actor update step.\n",
        "\n",
        "    def update_target_net(self):\n",
        "      # TODO: Implement target update step.\n",
        "\n",
        "\n",
        "    def update(self, step, num_iterations):\n",
        "        # TODO: Implement general update step (which calls other methods). Don't forget to print/return some useful information here."
      ],
      "metadata": {
        "id": "lf6G_O8jUiL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have implemented SAC, it is time to train and evaluate a policy. Below, we provide training hyperparameters which you should use in your experiments. See the writeup for additional instructions on what metrics and figures you need to include in your submission.\n",
        "\n",
        "* Expected Training Time (Colab CPU): < 5 minutes\n",
        "* Expected Episodic Reward: 1.6-1.7"
      ],
      "metadata": {
        "id": "cwu1QYNuHS20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to use the space below to run experiments and create plots used in your writeup.\n",
        "reseed(seed)\n",
        "env.reset(seed=seed)\n",
        "env.action_space.seed(seed)\n",
        "env.observation_space.seed(seed)\n",
        "\n",
        "sac_agent = SACAgent(env=env, gamma=0.99, tau=0.005, alpha = 0.2, actor_lr=3e-4, critic_lr=1e-3, network_hidden_dim=256)\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "\n",
        "losses = sac_agent.train(5000, 256)\n",
        "\n",
        "print(\"Average reward:\", evaluate(env, sac_agent, 10))\n",
        "\n",
        "visualize(env, algorithm=sac_agent, video_name=\"sac\")"
      ],
      "metadata": {
        "id": "26lqbqBaVu7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Please submit two separate plots in your writeup, of the value and policy loss across training. As well, note your average reward."
      ],
      "metadata": {
        "id": "PZSVDYIYCfAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to use this space for plotting."
      ],
      "metadata": {
        "id": "ABYQOsotCiX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "f80e6f76365c3a1e577e14ae81718f2e1aeab33a4811983ca756090a3b3aa652"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
