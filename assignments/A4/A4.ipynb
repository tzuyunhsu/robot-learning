{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlWmzCCILkQf"
      },
      "source": [
        "# **Assignment 4: Model Based Reinforcement Learning**\n",
        "\n",
        "### **Due Date**: 03/28/2025 at 11:59 PM\n",
        "\n",
        "### **Late Due Date**: 03/31/2025 at 11:59 PM\n",
        "\n",
        "#### **Writeup**: https://docs.google.com/document/d/1Ut7LWu_KagSjOGMFJwxMSRoVPtdlcUlSMQBOO7WkjPM/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwTW7FTsL6qV"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "Welcome to Assignment 4 of CS 4756/5756. In this assignment, you will train Policy Gradient methods under various World Models. This assignment is built up by the following components:\n",
        "\n",
        "- **[PROVIDED] Setup**: Dependency installing and initializations.\n",
        "- **[PROVIDED] Helper Functions**: Provided functions for visualization and evaluation.\n",
        "- **Part 1**: Train an expert PPO agent with StableBaselines3.\n",
        "- **Part 2**: Train a world model using environment transitions.\n",
        "- **Part 3**: Train a learner PPO agent on the world model.\n",
        "- **[GRAD] Part 4**: Aggregate new data from the learner policy.\n",
        "\n",
        "You will use the **FetchReach-v4** environment for this assignment. Refer to the Gymnasium-Robotics website for more details about this [environment](https://robotics.farama.org/envs/fetch/reach/)\n",
        "\n",
        "Please read through the following paragraphs carefully.\n",
        "\n",
        "**Getting Started**: You should complete this assignment on [Google Colab](https://colab.research.google.com).\n",
        "\n",
        "**Evaluation**: Your code will be tested for correctness and, for certain assignments, speed. For this particular assignment, performance results will not be harshly graded (although we provide approximate expected reward numbers, you are not expected to replicate them exactly). Please remember that all assignments should be completed individually.\n",
        "\n",
        "**Academic Integrity**: We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else’s code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don’t try. We trust you all to submit your own work only; please don’t let us down. If you do, we will pursue the strongest consequences available to us.\n",
        "\n",
        "**Getting Help**: The [Resources](https://www.cs.cornell.edu/courses/cs4756/2025sp/#resources) section on the course website is your friend! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. If you need a refresher for PyTorch, please see this [60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)! For Numpy, please see the quickstart [here](https://numpy.org/doc/stable/user/quickstart.html) and full API [here](https://numpy.org/doc/stable/reference/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdRJabMtQDy0"
      },
      "source": [
        "# **[PROVIDED] Setup**\n",
        "\n",
        "Please run the cells below to install the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Dl4AJ_MyQ6Nt"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "USING_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if USING_COLAB:\n",
        "    !apt-get -qq update\n",
        "    !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "else:\n",
        "    !pip install torch torchvision torchaudio\n",
        "    !pip install numpy\n",
        "    !pip install tqdm\n",
        "    !pip install opencv-python\n",
        "\n",
        "!pip install matplotlib\n",
        "!pip install -U mediapy\n",
        "!pip install -U renderlab\n",
        "!pip install -U \"imageio<3.0\"\n",
        "!pip install stable_baselines3\n",
        "\n",
        "!git clone https://github.com/Farama-Foundation/Gymnasium-Robotics.git\n",
        "!pip install -e Gymnasium-Robotics\n",
        "sys.path.append('/content/Gymnasium-Robotics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKWk__W9N0KN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Mujoco GLEW Setup\n",
        "try:\n",
        "    if _mujoco_run_once:  pass\n",
        "except NameError:\n",
        "    _mujoco_run_once = False\n",
        "\n",
        "if not _mujoco_run_once:\n",
        "    try:\n",
        "        os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "    except KeyError:\n",
        "        os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "\n",
        "    # Presetup so we don't see output on first env initialization\n",
        "    _mujoco_run_once = True\n",
        "    if USING_COLAB:\n",
        "        NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "        if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "            with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "                f.write(\"\"\"{\n",
        "                    \"file_format_version\" : \"1.0.0\",\n",
        "                    \"ICD\" : {\n",
        "                        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "                    }\n",
        "                }\"\"\")\n",
        "\n",
        "    # Set environment variable to support EGL (off-screen) rendering\n",
        "    %env MUJOCO_GL=egl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vynp2hcNO7mp"
      },
      "source": [
        "Please run the cells below to import necessary packages and set the initial seeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV-atSX6PFhy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import gymnasium.wrappers as wrappers\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.distributions as D\n",
        "from tqdm import tqdm, trange\n",
        "import torch.optim as optim\n",
        "import gymnasium_robotics\n",
        "import gymnasium as gym\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lygn5XbiPrDH"
      },
      "outputs": [],
      "source": [
        "seed = 695\n",
        "\n",
        "# Setting the seed to ensure reproducability\n",
        "def reseed(seed, env=None):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if env is not None:\n",
        "        env.unwrapped._np_random = gym.utils.seeding.np_random(seed)[0]\n",
        "\n",
        "reseed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TLmqmqePu4e"
      },
      "outputs": [],
      "source": [
        "# In this block we define wrappers necessary to simplify the environment MDP\n",
        "def wrap_reach_fixed_goal(env):\n",
        "    g = np.array([1.486, 0.73, 0.681], dtype=np.float32)\n",
        "    env.unwrapped._sample_goal = lambda: g\n",
        "    return env\n",
        "\n",
        "class FetchRewardWrapper(gym.Wrapper):\n",
        "    def reset(self, *args, **kwargs):\n",
        "        obs, info = self.env.reset(*args, **kwargs)\n",
        "        self.prev_dist = np.linalg.norm(obs['achieved_goal'] - obs['desired_goal'])\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action) # Terminated is never set to true\n",
        "        current_dist = np.linalg.norm(obs['achieved_goal'] - obs['desired_goal'])\n",
        "        reward = (self.prev_dist - current_dist) * 10\n",
        "        self.prev_dist = current_dist\n",
        "        return obs, reward, info['is_success'], truncated, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfIyvXTDRRyQ"
      },
      "source": [
        "# **[PROVIDED] Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js501GsJRzm0"
      },
      "source": [
        "### **Visualize Helper Function**\n",
        "\n",
        "Below, we provide the helper function `visualize` for your use. This function will create a visualization of the environment passed in the parameter `env`. If you are using Colab, calling this function will render the visualization within the notebook. If you are using your local machine, this function will instead save a video of the visualization to your current directory (rendering videos in Jupyter Notebooks is not widely supported outside of Colab).\n",
        "\n",
        "**Note:** In this code, a choice is provided on whether to vectorize the environment. The difference across vectorized and not vectorized gymnasium environments will be explained in the StableBaselines Introduction section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZXazZI4SN44"
      },
      "outputs": [],
      "source": [
        "def visualize(env: gym.Env, algorithm=None, video_name=\"test\"):\n",
        "    \"\"\"\n",
        "        Visualize a policy network for a given algorithm on a single episode\n",
        "\n",
        "        Args:\n",
        "            - env_name: Name of the gym environment to roll out `algorithm` in,\n",
        "                it will be instantiated using gym.make or make_vec_env.\n",
        "            - algorithm (PPOActor): Algorithm whose policy network will be rolled\n",
        "                out for the episode. If no algorithm is passed in, a random policy\n",
        "                will be visualized.\n",
        "            - video_name (str): Name for the mp4 file of the episode that will be\n",
        "                saved (omit .mp4). Only used when running on local machine.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_action(obs):\n",
        "        if not algorithm:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            return algorithm.select_action(obs)\n",
        "\n",
        "    if USING_COLAB:\n",
        "        import renderlab as rl\n",
        "\n",
        "        directory = './video'\n",
        "        env = rl.RenderFrame(env, \"output/\")\n",
        "        obs, info = env.reset()\n",
        "\n",
        "        for i in range(500):\n",
        "            action = get_action(obs)\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            if terminated or truncated: break\n",
        "        env.play()\n",
        "\n",
        "    else:\n",
        "        import cv2\n",
        "\n",
        "        video = cv2.VideoWriter(f\"{video_name}.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 24, (600,400))\n",
        "        obs = env.reset()\n",
        "\n",
        "        for i in range(500):\n",
        "            action = get_action(obs)\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            if terminated or truncated: break\n",
        "\n",
        "            im = env.render(mode='rgb_array')\n",
        "            im = im[:,:,::-1]\n",
        "            video.write(im)\n",
        "\n",
        "        video.release()\n",
        "        env.close()\n",
        "        print(f\"Video saved as {video_name}.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRIN6spvTDPc"
      },
      "source": [
        "### **Policy Evaluation Functions**\n",
        "\n",
        "The `evaluate_policy` function takes an agent actor, an environment whose output observations can be applied to the actor, and evaluates the policy by doing the following:\n",
        "\n",
        "- Rollout actor for a default of 100 trajectories, and record the total reward.\n",
        "- Return the average trajectory rewards over these episodes.\n",
        "\n",
        "**Note:** Since the actor we will be defining in this assignment exclusively uses a StableBaselines3 PPO agent, then the environment provided must be an instance of `VecEnv`, more information introduced in Part 1.\n",
        "\n",
        "The `success_rate` function is similar to the `evaluate_policy` function except that it takes a regular gymnasium environment instead of a vectorized environment. It also records the success rate as a percentage instead of the total reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9iI-hG7Tr3P"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(actor, environment, num_episodes=100, progress=True):\n",
        "    \"\"\"\n",
        "        Returns the mean trajectory reward of rolling out `actor` on `environment.\n",
        "\n",
        "        Parameters\n",
        "        - actor: PPOActor instance, defined in Part 1.\n",
        "        - environment: classstable_baselines3.common.vec_env.VecEnv instance.\n",
        "        - num_episodes: Total number of trajectories to collect and average over.\n",
        "    \"\"\"\n",
        "\n",
        "    total_rew = 0\n",
        "    iterate = (trange(num_episodes) if progress else range(num_episodes))\n",
        "\n",
        "    for _ in iterate:\n",
        "        obs = environment.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = actor.select_action(obs)\n",
        "            next_obs, reward, done, info = environment.step(action)\n",
        "            total_rew += reward\n",
        "            obs = next_obs\n",
        "\n",
        "    return (total_rew / num_episodes).item()\n",
        "\n",
        "\n",
        "def success_rate(actor, environment, num_episodes=100, progress=True):\n",
        "    \"\"\"\n",
        "        Returns the percentage of successful trajectories of `actor` on `environment`.\n",
        "\n",
        "        Parameters\n",
        "        - actor: PPOActor instance, defined in Part 1.\n",
        "        - environment: Gymnasium environment.\n",
        "        - num_episodes: Total number of trajectories to collect and average over.\n",
        "    \"\"\"\n",
        "\n",
        "    total_success = 0\n",
        "    iterate = (trange(num_episodes) if progress else range(num_episodes))\n",
        "\n",
        "    for _ in iterate:\n",
        "        obs, info = environment.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = actor.select_action(obs)\n",
        "            next_obs, reward, done, truncated, info = environment.step(action)\n",
        "            obs = next_obs\n",
        "\n",
        "            if done: total_success += 1\n",
        "            if truncated: break\n",
        "\n",
        "    return (total_success / num_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKdwYgcsUiiw"
      },
      "source": [
        "### **Notes About Fetch Reach Environment**\n",
        "\n",
        "The environment uses a Fetch Robot, which is a 7-DoF Mobile Manipulator.\n",
        "\n",
        "The task is a _goal-reaching task_: The observation space contains `observation` which includes the state of the robot in the environment, and `desired_goal` which specifies the xyz coordinate that the robot's gripper aims to reach.\n",
        "\n",
        "See https://robotics.farama.org/envs/fetch/reach/ for more details.\n",
        "\n",
        "If the goal is reached, `info['is_success']` will be set to 1, and this is an indication that we should terminate the rollout.\n",
        "\n",
        "The reward is normally -1 per timestep spent in the environment without completing the task, with 50 steps being the limit (so -50 is the worst episode return).\n",
        "\n",
        "> Note: For this assignment, we've modified the environment so that it only has a fixed goal to reach, and has better reward shaping. This is to make training easier and quicker later on.\n",
        "\n",
        "**Run the cells below to create and visualize the environment:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FMWnIEvVnff"
      },
      "outputs": [],
      "source": [
        "# Let's initialize the environment first\n",
        "reseed(seed)\n",
        "\n",
        "def make_fetch_env():\n",
        "    env = gym.make(\"FetchReach-v4\", render_mode=\"rgb_array\")\n",
        "    env = wrap_reach_fixed_goal(env)\n",
        "    env = FetchRewardWrapper(env)\n",
        "    env = wrappers.FilterObservation(env, [\"desired_goal\", \"observation\"])\n",
        "    env = wrappers.FlattenObservation(env)\n",
        "    return env\n",
        "\n",
        "real_env = make_fetch_env()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz7lc2RpVwxz"
      },
      "outputs": [],
      "source": [
        "visualize(real_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Oyrh2CFlh4_"
      },
      "source": [
        "# **Part 1: Train Expert Using StableBaselines3**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3zyZvwQmeeX"
      },
      "source": [
        "### **1.1: [PROVIDED] Introduction To Stable Baselines 3**\n",
        "\n",
        "StableBaselines3 is popular off-the-shelf set of reliable implementations of reinforcement learning algorithms in PyTorch. In this assignment, we will be using its PPO (Proximal Policy Gradient) implementation as our agent.\n",
        "\n",
        "Each algorithm implementation is a subclass of the `stable_baselines3.common.base_class.BaseAlgorithm` class, which provides us with the following functions:\n",
        "\n",
        "- `learn(total_timesteps, callback=None, log_interval=100, tb_log_name='run', reset_num_timesteps=True, progress_bar=False)`\n",
        "  - This is the training loop of any of the RL algorithm implementations. Training is done by calling this function with an appropriate amount of `total_timesteps`.\n",
        "- `predict(observation)`\n",
        "  - Returns a tuple `(predicted_action, next_hidden_state)` based on input `observation`. If we are not using an RNN, the next hidden state can be neglected.\n",
        "- `save(path)`\n",
        "  - Saves the current policy parameters into a `.zip` file with given `path`. Note that the `path` does not have the `.zip` postfix.\n",
        "- `load(path, env=None)`\n",
        "  - Loads a saved a `.zip` checkpoint into this RL implementation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSSJiWHto1on"
      },
      "source": [
        "### **1.2: [PROVIDED] Hyperparameters**\n",
        "\n",
        "The implementation has a set of hyperparameters that can be tuned towards better performance. For the sake of simplicity, we will provide the hyperparameters for the StableBaselines3 PPO implementation. The main ones we specify include the following:\n",
        "\n",
        "- `n_steps`: the number of steps to run with the environment for each update to the policy network.\n",
        "- `net_arch`: The network architecture of the policy network and the critic network:\n",
        "  - `pi`: a list that specifies the hidden dimensions of the policy network. The input and output dimension are determined by the environment associated with this policy.\n",
        "  - `vf`: a list that specifies the hidden dimensions of the critic network.\n",
        "  - `activation_fn`: Nonlinearity to be applied between each of the MLP layers.\n",
        "\n",
        "For a more comprehensive list and description of each of these hyperparameters, visit the official [documentation page](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters) for more information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iclE-kuMpflj"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.vec_env.base_vec_env import VecEnv\n",
        "\n",
        "hyperparameters = {\n",
        "    \"n_steps\": 512,\n",
        "    \"policy_kwargs\": {\n",
        "        \"net_arch\": {\n",
        "            \"pi\": [128],\n",
        "            \"vf\": [128],\n",
        "            \"activation_fn\": \"tanh\",\n",
        "        }\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aAGyJkErPai"
      },
      "source": [
        "### **1.3: Vectorized Environmnent**\n",
        "\n",
        "For any StableBaselines3 algorithm implementation, the gymnasium environment used need to be converted into a [vectorized environment](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#) of `VecEnv` type.\n",
        "\n",
        "A vectorized environment stacks multiple independent environments into one, stepping multiple `n` environments each time. If we set the the `n_envs` parameter to 3, then 3 environments will be stepped each time the VecEnv is stepped.\n",
        "\n",
        "**For the rest of this assignment, all vectorized environments with `n_env=n` will be described as n-vectorized.**\n",
        "\n",
        "With a vectorized environment that steps multiple environments at the same time, the model learning process can be made more efficient through parallelization trajectory collection across these independent environments. This `n_envs` parameter can be tailored to the specific machines.  \n",
        "\n",
        "**Important Differences:**\n",
        "- The vectorized environments now require input action to be a shape of `n_envs * act_dim`. The output observation from `step` and `reset` will also have the shape of `n_envs * obs_dim`.\n",
        "- The VecEnv `reset()` function returns only the observation, while the gymnasium.Env `reset()` function returns a tuple `(observation, info_dict)`.\n",
        "- The `vec_env.step(action)` function returns a 4-tuple of `(obs, reward, terminated, info)`, while the `gym_env.step(action)` returns a 5-tuple of `(obs, reward, terminated, truncated, info)`. The `terminated` value from VecEnv would equivalent to the gymnasium environment's `terminated or truncated`.\n",
        "\n",
        "\n",
        "A VecEnv instance can be created using the `make_vec_env` function, which takes the id of the wanted gymnasium environment, as well as the number of environments needed. This function has the following key parameters\n",
        "- `env_id`: the id of the gymnasium environment, or instantiated gym environment, or a callable that returns an env.\n",
        "- `n_envs`: The number of environments to have in parallel.\n",
        "- `seed`: The initial seed for the random number generator.\n",
        "- `env_kwargs`: An optional parameter to pass into the environment constructor.\n",
        "\n",
        "More detailed function documentation can be found in this [page](https://stable-baselines3.readthedocs.io/en/master/common/env_util.html#stable_baselines3.common.env_util.make_vec_env).\n",
        "\n",
        "**Instructions** For this part, please create two vectorized version of `FetchReach-v4` with 3 and 1 environments stacked. Note that because of our wrappers, you need to pass a callable, we have one called `make_fetch_env` defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDqfluapvTpq"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# TODO: Instantiate\n",
        "real_vec_env_1 = None\n",
        "real_vec_env_3 = None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj082PTVxQCF"
      },
      "source": [
        "### **1.4: Actor Definition**\n",
        "\n",
        "**Instruction**: You will need to implement the following PPOActor class, which serves as a wrapper to provide PPO model predictions.\n",
        "- `__init__`: Takes a path to the checkpoint and the corresponding environment, and load an instance of this PPO checkpoint. However if a PPO model is given, then the internally representing model uses that directly instead. This is for use in the Callback function, and since we provide that implementation for you, you will only need to implement the model loading portion of the constructor.\n",
        "- `select_action`: Takes an observation and produce the corresponding action prediction from the checkpoint PPO model. While implementing, take note of the output of the `predict` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lcJNUknx6gQ"
      },
      "outputs": [],
      "source": [
        "class PPOActor():\n",
        "    def __init__(self, ckpt: str=None, environment: VecEnv=None, model=None):\n",
        "        '''\n",
        "          Requires environment to be a 1-vectorized environment\n",
        "\n",
        "          The `ckpt` is a .zip file path that leads to the checkpoint you want\n",
        "          to use for this particular actor.\n",
        "\n",
        "          If the `model` variable is provided, then this constructor will store\n",
        "          that as the internal representing model instead of loading one from the\n",
        "          checkpoint path\n",
        "        '''\n",
        "        assert ckpt is not None or model is not None\n",
        "\n",
        "        if model is not None:\n",
        "            self.model = model\n",
        "            return\n",
        "\n",
        "        # TODO: Load checkpoint\n",
        "        self.model = None\n",
        "        # END TODO\n",
        "\n",
        "    def select_action(self, obs):\n",
        "        '''Gives the action prediction of this particular actor'''\n",
        "\n",
        "        # TODO: Select action\n",
        "        return None\n",
        "        # END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTb4OBjczJEK"
      },
      "source": [
        "### **1.5: [PROVIDED] Callbacks**\n",
        "\n",
        "To visualize the training process, since it could take a significant amount of time, StableBaselines3 provides a mean for us to visualize the training progress through a `BaseCallback` class instance, which can be optionally passed in as a parameter of the `learn` function. This Callback function is customizable by defining a subclass of `BaseCallback`.\n",
        "\n",
        "For this part, we provide you with a customized callback that evaluates the model under training every 1024 steps on an evaluating environment, which will be the 1-vectorized environment you have instantiated in the previous portion. Based on this evaluation result, this callback will save a checkpoint of the model if it is, so far, the best performing model. At the end of training, a plot of all evaluation results with respect to number of steps will be generated.\n",
        "\n",
        "You are free to modify this callback class to help you visualize training in any way most convenient for you, but is **NOT REQUIRED**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN1IdsrWzvpN"
      },
      "outputs": [],
      "source": [
        "class PPOCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0, save_path='default', eval_env=None):\n",
        "        super(PPOCallback, self).__init__(verbose)\n",
        "        self.rewards = []\n",
        "\n",
        "        self.save_freq = 1024\n",
        "        self.min_reward = -np.inf\n",
        "        self.actor = None\n",
        "        self.eval_env = eval_env\n",
        "\n",
        "        self.save_path = save_path\n",
        "        self.eval_steps = []\n",
        "        self.eval_rewards = []\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        self.actor = PPOActor(model=self.model)\n",
        "\n",
        "    def _on_rollout_start(self) -> None:\n",
        "        \"\"\"\n",
        "        A rollout is the collection of environment interaction\n",
        "        using the current policy.\n",
        "        This event is triggered before collecting new samples.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_rollout_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before updating the policy.\n",
        "        \"\"\"\n",
        "        episode_info = self.model.ep_info_buffer\n",
        "        rewards = [ep_info['r'] for ep_info in episode_info]\n",
        "        mean_rewards = np.mean(rewards)\n",
        "        self.rewards.append(mean_rewards)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        \"\"\"\n",
        "        This method will be called by the model after each call to `env.step()`.\n",
        "\n",
        "        For child callback (of an `EventCallback`), this will be called\n",
        "        when the event is triggered.\n",
        "\n",
        "        :return: If the callback returns False, training is aborted early.\n",
        "        \"\"\"\n",
        "        if self.eval_env is None:\n",
        "            return True\n",
        "\n",
        "        if self.num_timesteps % self.save_freq == 0 and self.num_timesteps != 0:\n",
        "            mean_reward = evaluate_policy(self.actor, environment=self.eval_env, num_episodes=20)\n",
        "            print(f'evaluating {self.num_timesteps=}, {mean_reward=}=======')\n",
        "\n",
        "            self.eval_steps.append(self.num_timesteps)\n",
        "            self.eval_rewards.append(mean_reward)\n",
        "            if mean_reward > self.min_reward:\n",
        "                self.min_reward = mean_reward\n",
        "                self.model.save(self.save_path)\n",
        "                print(f'model saved on eval reward: {self.min_reward}')\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        print(f'model saved on eval reward: {self.min_reward}')\n",
        "\n",
        "        plt.plot(self.eval_steps, self.eval_rewards, c='red')\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title('Rewards over Episodes')\n",
        "\n",
        "        plt.show()\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrLRUL8d0GjD"
      },
      "source": [
        "### **1.6 PPOActor Initialization And Training**\n",
        "\n",
        "The `stable_baselines3.ppo.PPO` class inherits from the `BaseAlgorithm` class described at the beginning of this section, and is specifically implemented for the PPO algorithm. To initialize a class, the following parameters are especially important:\n",
        "- `policy: str`: The policy type we use to train the agent, common ones include MlpPolicy and CnnPolicy. In our case, we will be using the MlpPolicy.\n",
        "- `env: VecEnv`: The environment that the agent rollouts on for training, must be vectorized or it will be vectorized by the PPO implementation\n",
        "- `n_steps`: number of steps to optimize the policy for\n",
        "- `device`: The device to put the model on (For this assignment, if you're not able to reach the performance bounds, try setting this parameter to cpu)\n",
        "- Other hyperparameters specified in the `hyperparameters` dictionary we provided, can be directly applied using the `**` operator.\n",
        "\n",
        "**Instructions**\n",
        "- Initialize a PPO MLP policy as expert, using the 3-env VecEnv initialized in the previous part and pass in the given hyperparameters.\n",
        "- Train the expert with an instance of the `PPOCallback` defined before. No need to save the resulting model into checkpoint since that is done for you in the Callback class\n",
        "  - (HINT): Look at the beginning of Part 1 for useful functions for training.\n",
        "\n",
        "\n",
        "**Estimated Training Time**:\n",
        "- 2 - 4 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9sZSN8E0nsb"
      },
      "outputs": [],
      "source": [
        "reseed(seed)\n",
        "ckpt_path = 'expert'\n",
        "total_steps = 40960\n",
        "expert_callback = PPOCallback(save_path=ckpt_path, eval_env=real_vec_env_1)\n",
        "\n",
        "# TODO: Instantiate and train\n",
        "expert = None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZC4sPko4SNj"
      },
      "source": [
        "### **1.7: Evaluate Expert**\n",
        "\n",
        "**Instructions** Initialize an expert PPOActor instance from the checkpoint and evaluate the expert agent using the `evaluate_policy` and `success_rate` function on the real environment.\n",
        "\n",
        "**Expected Reward**: Around 1.6 - 1.7 on `real_vec_env_1`\n",
        "\n",
        "**Expected Success**: Around 0.95 on `real_env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS955D_N4fjw"
      },
      "outputs": [],
      "source": [
        "expert = PPOActor(ckpt_path, real_vec_env_1)\n",
        "\n",
        "# TODO: Evaluate\n",
        "\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW4bsnsJ-d83"
      },
      "source": [
        "### **1.8: [PROVIDED] Visualize Expert**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyfbh1SW-giO"
      },
      "outputs": [],
      "source": [
        "visualize(real_env, algorithm=expert, video_name='expert')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtHsRPJNKWVI"
      },
      "source": [
        "# **Part 2: Collect Data And Train World Model**\n",
        "\n",
        "### **2.1: [PROVIDED] Overview**\n",
        "Unlike in simulation, we can rarely obtain the full transition function of real world scenarios, and we emulate that property in this assignment here.\n",
        "\n",
        "Assuming we do not have the underlying logic to the `FetchReach-v4`, given that we have an expert agent in solving this particular problem, we take the following model based reinforcement learning approach to learn an RL agent that can be applied to the real scenario.\n",
        "\n",
        "In real life, we might not have such a trained expert, and human operating the robot remotely could be one source of expert data.\n",
        "\n",
        "1. Rollout a series of expert trajectories in the true environment (analogous to collecting a set of human demonstrations on the robot)\n",
        "2. Define and train a world model with the trajectory transitions as input data\n",
        "3. Define a new environment that applies the trained world model\n",
        "4. Learn an RL agent under the learned environment\n",
        "5. Evaluate this agent using the real environment\n",
        "\n",
        "You will need to implement the following functions and classes\n",
        "- `data_collect`: a helper function that rolls out a policy on an environment, and returning a tuple of lists representing the transitions\n",
        "- `WorldModel` : a `torch.nn` module defining the architecture of the world.\n",
        "- `train_world_model` and `eval_world_model`: Training and evaluation loop of the world model\n",
        "\n",
        "Follow the instructions below to implement each of these components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvr9fhVJLW5h"
      },
      "source": [
        "### **2.2: Collect Data**\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "The `data_collect` function should rollout a policy actor on the environment for a total of `num_steps`, with a maximum trajectory length of `traj_max_length`, then returning 3 lists: `observations`, `actions`, `next_observations` such that for any transition $i \\leq$ num_steps:\n",
        "\n",
        "data_env with initial state `observations[i]`, when stepped with `actions[i]`, yields a new state `next_observations[i]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjQrW5tkMHmq"
      },
      "outputs": [],
      "source": [
        "def data_collect(num_steps: int, traj_max_length: int, data_env: gym.Env, actor: PPOActor):\n",
        "    '''\n",
        "    Collects observation, action, next_observation triplet data for `num_trajectories`\n",
        "    each with a maximimum step count of `traj_max_length`\n",
        "\n",
        "    - num_steps: Number of total steps to collect data over, should also be the sum of trajectory lengths\n",
        "    - traj_max_length: Maximum length of each trajectory\n",
        "    - data_env: The environment to collect data under, NOT A VecEnv\n",
        "\n",
        "    - actor: A function that takes a `data_env` observation as input and outputs an action admissible to `data_env`\n",
        "\n",
        "    Returns: (observations, actions, next_observations), each being a list\n",
        "    '''\n",
        "\n",
        "    observations, actions, next_obs = [], [], []\n",
        "\n",
        "    # TODO: Step and collect data\n",
        "\n",
        "    # END TODO\n",
        "\n",
        "    return observations, actions, next_obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYbPL7WnNpmK"
      },
      "source": [
        "**Instructions**\n",
        "Run data collection function on the real environment with the expert policy trained in part 1.\n",
        "\n",
        "**Note**: The `data_collect` function requires the environment provided to be a regular gymnasium environment instead of a vectorized environment. Please make sure to not confuse it with `real_vec_env_1` defined in part 1.1.\n",
        "\n",
        "**Note**: Here is a list of currently created environments:\n",
        "- `real_env`\n",
        "- `real_vec_env_1`\n",
        "- `real_vec_env_3`\n",
        "\n",
        "Refer to function documentation for selecting which one to use when doing function calls.\n",
        "\n",
        "**Estimated Collection Time**:\n",
        "- 2 - 4 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBW7vmiwNzSz"
      },
      "outputs": [],
      "source": [
        "total_steps = 50000\n",
        "traj_max_length = 500\n",
        "reseed(seed, env=real_env)\n",
        "\n",
        "# TODO: Collect data\n",
        "observations, actions, next_obs = None, None, None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPfR6BhcOSFG"
      },
      "source": [
        "### **2.3: [PROVIDED] Visualize And Create Dataset**\n",
        "\n",
        "**Note** The below visualization is showing multiple coordinates of the observation at the same time, so it looks a bit weird. You should see 3 distinct curves, where each curve is made of overlapping red and blue components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuce42YbOnMn"
      },
      "outputs": [],
      "source": [
        "def visualize_collected_data(observations, next_obs):\n",
        "    '''\n",
        "        Takes the first 300 data points and generates a plot of the observations and next_obs.\n",
        "    '''\n",
        "    print(f'Dataset Size: {len(observations)}')\n",
        "    print(f'Observation Size: {observations[0].shape}')\n",
        "    plt.close()\n",
        "    plt.plot(np.arange(300), [obs[3:6] for obs in observations[:300]], c='blue')\n",
        "    plt.plot(np.arange(300), [obs[3:6] for obs in next_obs[:300]], c='red')\n",
        "    plt.show()\n",
        "\n",
        "visualize_collected_data(observations, next_obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8nA9urZPh1p"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class WorldDataset(Dataset):\n",
        "    def __init__(self, obs, actions, next_obs):\n",
        "        self.obs = obs\n",
        "        self.actions = actions\n",
        "        self.next_obs = next_obs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.obs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'orig_obs': self.obs[idx],\n",
        "            'action': self.actions[idx],\n",
        "            'next_obs': self.next_obs[idx]\n",
        "        }\n",
        "\n",
        "split = len(observations) // 5\n",
        "val_data = WorldDataset(observations[:split], actions[:split], next_obs[:split])\n",
        "train_data = WorldDataset(observations[split:], actions[split:], next_obs[split:])\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=128)\n",
        "val_dataloader = DataLoader(val_data, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHd92p1WP7Wr"
      },
      "source": [
        "### **2.4: Define World Model**\n",
        "\n",
        "The `WorldModel` class should define a neural network that takes a state-action pair and outputs a state in the state space. The network should have the following architecture (the type of these layers should be `torch.float64`).\n",
        "\n",
        "- Layer 1: a fully-connected layer with `inp_dim` input nodes and `hidden_dim_1` output nodes, followed by a ReLU activation function.\n",
        "- Layer 2: a fully-connected layer with `hidden_dim_1` input nodes and `hidden_dim_2` output nodes, followed by another ReLU.\n",
        "- Output layer: a fully-connected layer with `hidden_dim_2` input nodes and `output_dim` output nodes.\n",
        "\n",
        "The `forward` function should take two inputs: `state` and `action`, concatenate them along the last dimension, and then pass it through the model architecture. For instance, if the state has shape `n * s`, and the action has shape `n * a`, then the input to the model should be `n * (s + a)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZC7pzilQhDH"
      },
      "outputs": [],
      "source": [
        "class WorldModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim):\n",
        "        super(WorldModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # TODO: Define architecture\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.fc3 = None\n",
        "        # END TODO\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        '''\n",
        "            Expected `state` to have shape n * s_dim\n",
        "            Expected `action` to have shape n * a_dim\n",
        "        '''\n",
        "        n, s_dim = state.shape\n",
        "        n_a, a_dim = action.shape\n",
        "        assert n == n_a\n",
        "        assert s_dim + a_dim == self.input_dim\n",
        "\n",
        "        # TODO: Calculate next state\n",
        "        return None\n",
        "        # END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dpxig7rQvNX"
      },
      "source": [
        "### **2.5: Training And Validation Function For World Model**\n",
        "\n",
        "The `train_world_model` function should train the provided model for one epoch, using the optimizer and criterion provided on the given train_dataloader. This function should iterate through each batch of the `train_dataloader` once, update the world model based on the loss calculated by criterion, then step the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_DV5VMoRISi"
      },
      "outputs": [],
      "source": [
        "def train_world_model(model, optimizer, criterion, train_dataloader):\n",
        "    '''\n",
        "        This function should train the torch model `model` using the\n",
        "        optim `optimizer` and `criterion` as loss function, on one pass\n",
        "        of the `train_dataloader`\n",
        "\n",
        "        This is should train the model for on epoch, as in one pass through\n",
        "        the training data.\n",
        "\n",
        "        Returns: the mean criterion loss across each batch of the dataset.\n",
        "    '''\n",
        "    total_loss, cnt = 0, 0\n",
        "    model.train()\n",
        "\n",
        "    # TODO: Update the model for one epoch\n",
        "\n",
        "    # END TODO\n",
        "\n",
        "    return total_loss / cnt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrM5vqkuRVkh"
      },
      "source": [
        "The `eval_world_model` function is similar to the `train_world_model` function with iteration through the batches of the `eval_dataloader` and computes the loss using the given criterion. Note that no update to model should be made and gradients should not be calculated during the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXN0egO-Woeu"
      },
      "outputs": [],
      "source": [
        "def eval_world_model(model, criterion, eval_dataloader):\n",
        "    '''\n",
        "        This function should evaluate the torch model `model` using\n",
        "        `criterion` as loss function, on one pass of the `eval_dataloader`\n",
        "\n",
        "        This is should evaluate the model on the validation dataset.\n",
        "\n",
        "        Take note that during evaluation, the model should not be updated\n",
        "        in any way and gradients should not be calculated.\n",
        "\n",
        "        Returns: the mean criterion loss across each batch of the dataset.\n",
        "\n",
        "    '''\n",
        "    total_loss, cnt = 0, 0\n",
        "    model.eval()\n",
        "\n",
        "    # TODO: Evaluate the model across the whole dataset\n",
        "\n",
        "    # END TODO\n",
        "\n",
        "    return total_loss / cnt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R9pfDPfXG7c"
      },
      "source": [
        "### **2.6: Train The World Model**\n",
        "\n",
        "Train an instance of `WorldModel` for `50` epochs with the dataloader built in previous section, using Adam optimizer and MSE loss, with an `lr` of `0.0001`. Provide a plot of training and evaluation losses with respect to training epochs, and also print out the final evaluation loss.\n",
        "\n",
        "**Estimated Training Time**:\n",
        "- 2 - 4 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhiyMTPTXih2"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "reseed(seed)\n",
        "lr = 0.0001\n",
        "\n",
        "world_model = WorldModel(input_dim=17, hidden_dim_1=32, hidden_dim_2=64, output_dim=13)\n",
        "optimizer = torch.optim.Adam(world_model.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# TODO: Train and evaluate world model\n",
        "train_losses, eval_losses = [], []\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV1lCAFGbOyK"
      },
      "source": [
        "### **2.7: [PROVIDED] Build Gym Environment With World Model**\n",
        "\n",
        "The following `WorldModelEnv` class is largely defined for you to train your next PPO agent as the reinforcement learning component of MBRL. In this environment, the reward is calculated the same as `real_env`\n",
        "\n",
        "To initialize a `WorldModelEnv` environment, a `world_model` (an instance of WorldModel in this case) should be passed in as argument, which will be used as the transition function in the `step()` function.\n",
        "\n",
        "This environment is registered with an id of **WorldModelFetch**, which can be initialized using `gym.make` or directly initializing it.\n",
        "\n",
        "\n",
        "**Run the following cell to define and register this environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhfnvQTKbmC6"
      },
      "outputs": [],
      "source": [
        "class WorldModelEnv(gym.Env):\n",
        "    def __init__(self, world_model: WorldModel, render_mode: str='rgb_array'):\n",
        "        super(WorldModelEnv, self).__init__()\n",
        "        self.metadata = { 'render_modes': ['human', 'rgb_array'], 'render_fps': 30 }\n",
        "        self.render_mode = 'rgb_array'\n",
        "        self.world_model = world_model\n",
        "        self.corr_env = real_env\n",
        "\n",
        "        self.observation_space = self.corr_env.observation_space\n",
        "        self.action_space = self.corr_env.action_space\n",
        "        self.obs_min = self.corr_env.observation_space.low\n",
        "        self.obs_max = self.corr_env.observation_space.high\n",
        "\n",
        "        self.state = self.corr_env.reset()[0]\n",
        "        self.goal_position = self.state[:3]\n",
        "        self.prev_dist = np.linalg.norm(self.state[3:6] - self.goal_position)\n",
        "        self.step_count = 0\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        pass\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.state = self.corr_env.reset()[0]\n",
        "        self.goal_position = self.state[:3]\n",
        "        self.prev_dist = np.linalg.norm(self.state[3:6] - self.goal_position)\n",
        "        self.step_count = 0\n",
        "        return self.state, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        with torch.no_grad():\n",
        "            state = torch.from_numpy(self.state).unsqueeze(0)\n",
        "            action = torch.from_numpy(action).unsqueeze(0)\n",
        "            next_state = self.world_model(state, action).squeeze(0).numpy()\n",
        "            self.state = np.clip(next_state, a_min=self.obs_min, a_max=self.obs_max)\n",
        "\n",
        "        current_dist = np.linalg.norm(self.state[3:6] - self.goal_position)\n",
        "        reward = (self.prev_dist - current_dist) * 10\n",
        "        self.prev_dist = current_dist\n",
        "        self.step_count += 1\n",
        "\n",
        "        terminated = self.corr_env.unwrapped._is_success(self.state[3:6], self.goal_position)\n",
        "        truncated = self.step_count >= 500\n",
        "        return self.state, reward, bool(terminated), truncated, {}\n",
        "\n",
        "gym.register(id='WorldModelFetch', entry_point=WorldModelEnv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW_1DUv8d-ub"
      },
      "source": [
        "# **Part 3: Train Agent On World Model**\n",
        "\n",
        "Now that we have learned a model that simulates the transition function of the real environment, it's time to train an agent on this model. In this part, you will learn, visualize, and evaluate a PPO policy on the learned world model, similar to what happened in Part 1, using functions defined in the Helper Function section, Part 1, and Part 2.\n",
        "\n",
        "**Follow instructions to complete each component**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIqreIRleV08"
      },
      "source": [
        "### **3.1: Train New PPO On World Model**\n",
        "\n",
        "**Instruction 1** Initialize a 3-vectorized and a 1-vectorized `WorldModelFetch` environment.\n",
        "\n",
        "**Instruction 2** For this part, we will train a separate PPO policy using the learned model environment. This model should be trained with the world model learned in the previous part for 40960 steps, under a 3-vectorized environment, using the same hyperparameters provided in Part 1.\n",
        "\n",
        "**Note**: Here is a list of created environments after running this following cell:\n",
        "- `real_env`\n",
        "- `real_vec_env_1`\n",
        "- `real_vec_env_3`\n",
        "- `world_vec_env_1`\n",
        "- `world_vec_env_3`\n",
        "\n",
        "Refer to function documentation for selecting which one to use when doing function calls.\n",
        "\n",
        "**Estimated Training Time**:\n",
        "- 2 - 4 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTCLqMwCfD2z"
      },
      "outputs": [],
      "source": [
        "learner_ckpt_path = 'learner'\n",
        "total_steps = 40960\n",
        "reseed(seed)\n",
        "\n",
        "# TODO 1: Create vectorized world environments (HINT: use env_kwargs)\n",
        "env_kwargs = None\n",
        "world_vec_env_1 = None\n",
        "world_vec_env_3 = None\n",
        "# END TODO\n",
        "\n",
        "learner_callback = PPOCallback(save_path=learner_ckpt_path, eval_env=world_vec_env_1)\n",
        "\n",
        "# TODO 2: Initiate training\n",
        "learner = None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulDxTvi4f7_9"
      },
      "source": [
        "### **3.2: [PROVIDED] Visualize Learned Policy On Real Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c7E54HugGn_"
      },
      "outputs": [],
      "source": [
        "learner_actor = PPOActor(ckpt=f'{learner_ckpt_path}.zip', environment=world_vec_env_1)\n",
        "visualize(real_env, algorithm=learner_actor, video_name=\"learner_eval\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Ja-ydighdH"
      },
      "source": [
        "### **3.3: Evaluate Learned Policy**\n",
        "\n",
        "Evaluate the learner agent on both the learned world model and the real environment. Also print out the success rate on the real environment.\n",
        "\n",
        "**Expected Rewards**:\n",
        "- 1.6 - 1.7 on `world_vec_env_1`\n",
        "- 0.7 - 0.9 on `real_vec_env_1`\n",
        "\n",
        "**Expected Success**: Around 0.5 on `real_env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1OBS_negxBi"
      },
      "outputs": [],
      "source": [
        "learner = PPOActor(ckpt=f'{learner_ckpt_path}.zip', environment=world_vec_env_1)\n",
        "\n",
        "# TODO: Evaluate on both environments\n",
        "\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I9zI8dEqO8E"
      },
      "source": [
        "# **[GRAD] Part 4: World Model With Aggregated Data**\n",
        "\n",
        "### **4.1: Collect Data With Learner Policy**\n",
        "\n",
        "In this section, we collect another 50000 steps of data with the learned agent from the previous part on the real environment, simulating rollouts of the learned agent in real world.\n",
        "\n",
        "**Estimated Collection Time**:\n",
        "- 2 - 4 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAS7wxuXqq1u"
      },
      "outputs": [],
      "source": [
        "total_steps = 50000\n",
        "traj_max_length = 500\n",
        "\n",
        "# TODO: Collect data using learner\n",
        "policy_obs, policy_acts, policy_next_obs = None, None, None\n",
        "# END TODO\n",
        "\n",
        "visualize_collected_data(policy_obs, policy_next_obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq0nVkarr2w2"
      },
      "source": [
        "### **4.2: Aggregate Data**\n",
        "\n",
        "**Instruction**: Aggregate the expert data collect in Part 2 and the agent data collected from previous section, shuffle their order, and form a new train/val split instances of the `WorldDataset` and dataloaders. The train / val split should be **80% / 20%**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEx6EJ1CsGWy"
      },
      "outputs": [],
      "source": [
        "# TODO: Aggregate data and split into training and validation\n",
        "agg_train_dataloader = None\n",
        "agg_val_dataloader = None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZIdLrM2teiS"
      },
      "source": [
        "### **4.3: Train Second World Model Using Aggregate Data**\n",
        "\n",
        "- Train for 50 epochs with `lr=0.0001`, Adam optimizer, and MSELoss criterion.\n",
        "- Save the train and validation losses across epochs and plot them\n",
        "\n",
        "**Estimated Training Time**:\n",
        "- 3 - 5 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1eTf3dlt_w2"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "reseed(seed)\n",
        "lr = 0.0001\n",
        "\n",
        "agg_world_model = WorldModel(input_dim=17, hidden_dim_1=32, hidden_dim_2=64, output_dim=13)\n",
        "agg_optimizer = torch.optim.Adam(agg_world_model.parameters(), lr=lr)\n",
        "agg_criterion = nn.MSELoss()\n",
        "\n",
        "# TODO: Train and evaluate 2nd world model\n",
        "agg_train_losses, agg_eval_losses = [], []\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5j44x48wFA5"
      },
      "source": [
        "### **4.4: Train PPO Using Second World Model**\n",
        "\n",
        "**Instruction 1** Initialize a 3-vectorized and a 1-vectorized WorldModelFetch environment with the world model trained on aggregated dataset\n",
        "\n",
        "**Instruction 2** Train a separate PPO policy using the world model learned on aggregated data. Similar as before, this model should be trained for 40960 steps, under a 3-vectorized environment, using the same hyperparameters provided in Part 1.\n",
        "\n",
        "**Note**: Here is a list of created environments after running this following cell:\n",
        "- `real_env`\n",
        "- `real_vec_env_1`\n",
        "- `real_vec_env_3`\n",
        "- `world_vec_env_1`\n",
        "- `world_vec_env_3`\n",
        "- `agg_world_vec_env_1`\n",
        "- `agg_world_vec_env_3`\n",
        "\n",
        "Refer to function documentation for selecting which one to use when doing function calls.\n",
        "\n",
        "**Estimated Training Time**:\n",
        "- 3 - 5 minutes on Google Colab CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV1gZSa9w8K2"
      },
      "outputs": [],
      "source": [
        "agg_ckpt_path = 'agg_learner'\n",
        "total_steps = 40960\n",
        "reseed(seed)\n",
        "\n",
        "# TODO 1: Create vectorized world environments (HINT: use env_kwargs)\n",
        "env_kwargs = None\n",
        "agg_world_vec_env_1 = None\n",
        "agg_world_vec_env_3 = None\n",
        "# END TODO\n",
        "\n",
        "agg_callback = PPOCallback(save_path=agg_ckpt_path, eval_env=agg_world_vec_env_1)\n",
        "\n",
        "# TODO 2: Initiate training\n",
        "agg_learner = None\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.5: [PROVIDED] Visualize Learned Policy On Real Environment**"
      ],
      "metadata": {
        "id": "b_O5AP4AsSKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_actor = PPOActor(ckpt=f'{agg_ckpt_path}.zip', environment=agg_world_vec_env_1)\n",
        "visualize(real_env, algorithm=agg_actor, video_name=\"agg_eval\")"
      ],
      "metadata": {
        "id": "xcSgUmfssYz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EZ4iE8bxiGO"
      },
      "source": [
        "### **4.6: Evaluate PPO On Real And Second World Environment**\n",
        "\n",
        "**Expected Rewards:**\n",
        "- About 1.6 to 1.7 on `agg_world_vec_env_1`\n",
        "- About 1.5 to 1.7 on `real_vec_env_1`\n",
        "\n",
        "**Expected Success:** Around 0.85 on `real_env`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi3nCT2xyOeT"
      },
      "outputs": [],
      "source": [
        "agg_learner = PPOActor(ckpt=f'{agg_ckpt_path}.zip', environment=agg_world_vec_env_1)\n",
        "\n",
        "# TODO: Evaluate on both environments.\n",
        "\n",
        "# END TODO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}