{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GugcHV0e5IKo"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "Welcome to the programming portion of assignment 2 of CS 4756/5756. In this assignment, you will implement several variations of Policy Gradient methods. Concretely, you will:\n",
        "* Implement the REINFORCE algorithm (Part 1)\n",
        "* Incorporate Reward-To-Go into the REINFORCE loss function (Part 2)\n",
        "\n",
        "You will use the FetchReach-v4 environment for this assignment. Refer to the Gymnasium-Robotics website for more details about [this environment](https://robotics.farama.org/envs/fetch/reach/).\n",
        "\n",
        "\n",
        "Please read through the following paragraphs carefully.\n",
        "\n",
        "**Getting Started:** You should complete this assignment on **[Google Colab](https://colab.research.google.com/)**.\n",
        "\n",
        "**Evaluation:**\n",
        "Your code will be tested for correctness and, for certain assignments, speed. For this particular assignment, performance results will not be harshly graded (although we provide approximate expected reward numbers, you are not expected to replicate them exactly). Please remember that all assignments should be completed individually.\n",
        "\n",
        "**Academic Integrity:** We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else’s code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don’t try. We trust you all to submit your own work only; please don’t let us down. If you do, we will pursue the strongest consequences available to us.\n",
        "\n",
        "**Getting Help:** The [Resources](https://www.cs.cornell.edu/courses/cs4756/2025sp/#resources) section on the course website is your friend! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. If you need a refresher for PyTorch, please see this [60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)! For Numpy, please see the quickstart [here](https://numpy.org/doc/stable/user/quickstart.html) and full API [here](https://numpy.org/doc/stable/reference/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d52C6fHT9eR1"
      },
      "source": [
        "# **Setup**\n",
        "\n",
        "**Please run the cells below to install the necessary packages**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pEa_6lD3-OHI",
        "outputId": "31736524-680c-4260-caa9-cbace096528f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "USING_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if USING_COLAB:\n",
        "  !apt-get -qq update\n",
        "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "else:\n",
        "  !pip install torch torchvision torchaudio\n",
        "  !pip install numpy\n",
        "  !pip install tqdm\n",
        "  !pip install opencv-python\n",
        "\n",
        "!pip install matplotlib\n",
        "!pip install -U mediapy\n",
        "!pip install -U renderlab\n",
        "!pip install -U \"imageio<3.0\"\n",
        "!git clone https://github.com/Farama-Foundation/Gymnasium-Robotics.git\n",
        "!pip install -e Gymnasium-Robotics\n",
        "sys.path.append('/content/Gymnasium-Robotics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5LxCSgkZJgL",
        "outputId": "ff9d3539-4fb6-419a-c994-a6a72db3e227"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Mujoco GLEW Setup\n",
        "try:\n",
        "  if _mujoco_run_once:\n",
        "    pass\n",
        "except NameError:\n",
        "  _mujoco_run_once = False\n",
        "if not _mujoco_run_once:\n",
        "  try:\n",
        "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  except KeyError:\n",
        "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  # presetup so we don't see output on first env initialization\n",
        "  _mujoco_run_once = True\n",
        "  if USING_COLAB:\n",
        "    NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "    if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "      with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "          f.write(\"\"\"{\n",
        "          \"file_format_version\" : \"1.0.0\",\n",
        "          \"ICD\" : {\n",
        "              \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "          }\n",
        "      }\n",
        "      \"\"\")\n",
        "  # Set environment variable to support EGL (off-screen) rendering\n",
        "  %env MUJOCO_GL=egl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhHPfeX8SdVy"
      },
      "source": [
        "# Import packages and initial seeding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7QH1iD8ZDeZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as D\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import gymnasium_robotics\n",
        "import gymnasium.wrappers as wrappers\n",
        "import random\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QvsVcrNZFem"
      },
      "outputs": [],
      "source": [
        "seed = 695\n",
        "\n",
        "# Setting the seed to ensure reproducability\n",
        "def reseed(seed : int):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "reseed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7NJsMDOawJd"
      },
      "outputs": [],
      "source": [
        "# In this block we define wrappers necessary to simplify the environment MDP\n",
        "def wrap_reach_fixed_goal(env):\n",
        "  g = np.array([1.486, 0.73, 0.681], dtype=np.float32)\n",
        "  env.unwrapped._sample_goal = lambda: g\n",
        "  return env\n",
        "\n",
        "class FetchRewardWrapper(gym.Wrapper):\n",
        "  def reset(self, *args, **kwargs):\n",
        "    obs, info = self.env.reset(*args, **kwargs)\n",
        "    self.prev_dist = np.linalg.norm(obs['achieved_goal'] - obs['desired_goal'])\n",
        "    return obs, info\n",
        "\n",
        "  def step(self, action):\n",
        "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "    achieved_goal = obs['achieved_goal']\n",
        "    desired_goal = obs['desired_goal']\n",
        "    current_dist = np.linalg.norm(achieved_goal - desired_goal)\n",
        "    reward = (self.prev_dist - current_dist) * 10\n",
        "    self.prev_dist = current_dist\n",
        "    return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq62xgt0_q_3"
      },
      "source": [
        "# Visualize Helper Function\n",
        "\n",
        "Below, we provide the helper function `visualize` for your use. This function will create a visualization of the environment passed in the parameter `env`. If you are using Colab, calling this function will render the visualization within the notebook. If you are using your local machine, this function will instead save a video of the visualization to your current directory (rendering videos in Jupyter Notebooks is not widely supported outside of Colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlAWmVPoBJz4"
      },
      "outputs": [],
      "source": [
        "def visualize(env : gym.Env, algorithm=None, video_name=\"test\"):\n",
        "    \"\"\"Visualize a policy network for a given algorithm on a single episode\n",
        "\n",
        "        Args:\n",
        "            algorithm (PolicyGradient): Algorithm whose policy network will be rolled out for the episode. If\n",
        "            no algorithm is passed in, a random policy will be visualized.\n",
        "            video_name (str): Name for the mp4 file of the episode that will be saved (omit .mp4). Only used\n",
        "            when running on local machine.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_action(obs):\n",
        "        if not algorithm:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            return algorithm.compute_action(obs)\n",
        "\n",
        "    if USING_COLAB:\n",
        "        import renderlab as rl\n",
        "\n",
        "        directory = './video'\n",
        "        env = rl.RenderFrame(env, \"output/\")\n",
        "        obs, info = env.reset()\n",
        "        for i in range(500):\n",
        "            action = get_action(obs)\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        env.play()\n",
        "    else:\n",
        "        import cv2\n",
        "\n",
        "        video = cv2.VideoWriter(f\"{video_name}.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 24, (600,400))\n",
        "        obs = env.reset()\n",
        "        for i in range(500):\n",
        "            action = get_action(obs)\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "            im = env.render(mode='rgb_array')\n",
        "            im = im[:,:,::-1]\n",
        "\n",
        "            video.write(im)\n",
        "\n",
        "        video.release()\n",
        "        env.close()\n",
        "        print(f\"Video saved as {video_name}.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u322aMAspfpr"
      },
      "source": [
        "## **Notes about Fetch Reach Environment**\n",
        "The environment uses a Fetch Robot, which is a 7-DoF Mobile Manipulator.\n",
        "\n",
        "The task is a *goal-reaching task*:\n",
        "The observation space contains '`observation`' which includes the state of the robot in the environment, and '`desired_goal`' which specifies the xyz coordinate that the robot's gripper aims to reach.\n",
        "\n",
        "See https://robotics.farama.org/envs/fetch/reach/ for more details.\n",
        "\n",
        "\n",
        "If the goal is reached, `info['is_success']` will be set to 1, and this is an indication that we should terminate the rollout.\n",
        "\n",
        "\n",
        "The reward is -1 per timestep spent in the environment without completing the task, with 50 steps being the limit (so -50 is the worst episode return).\n",
        "\n",
        "> Note: For this assignment, we've modified the environment that it only has a fixed goal to reach, and has better reward shaping, since vanilla REINFORCE will produce very noisy gradients for complex markov chains.\n",
        "\n",
        "**Run the cell below to create and visualize the environment:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoqC_reVB9hM"
      },
      "outputs": [],
      "source": [
        "# Let's initialize the environment first\n",
        "reseed(seed)\n",
        "env = gym.make(\"FetchReach-v4\", render_mode=\"rgb_array\")\n",
        "env = wrap_reach_fixed_goal(env)\n",
        "env = FetchRewardWrapper(env)\n",
        "env = wrappers.FilterObservation(env, [\"desired_goal\", \"observation\"])\n",
        "env = wrappers.FlattenObservation(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "xi_4XxI__m4u",
        "outputId": "7839ab89-9f08-4b27-ef33-e1704e669352"
      },
      "outputs": [],
      "source": [
        "visualize(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H9zQBy_zg0X"
      },
      "source": [
        "# Part 1: Vanilla REINFORCE\n",
        "\n",
        "In this assignment, we will implement the REINFORCE algorithm and apply it to our environment. This algorithm is a popular reinforcement learning algorithm that can be used for both discrete and continuous action spaces.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The REINFORCE algorithm uses a neural network to learn a policy that maps states to actions. The algorithm collects a set of trajectories, which are used to update the policy network. Below, we present a brief overview of REINFORCE and the equations behind it. **See MACRL 11.4 for a full description of REINFORCE and derivations for the following equations.**\n",
        "\n",
        "To start, recall the reinforcement learning objective:\n",
        "$$ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[r(\\tau)] $$\n",
        "\n",
        "\n",
        "The goal of REINFORCE is to maximize $J(\\theta)$. As REINFORCE is a policy gradient algorithm, it involves taking the gradient of this objective with respect to the parameters $\\theta$ of the policy $\\pi_{\\theta}$:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\nabla_θ log \\pi_\\theta (\\tau) r(\\tau)] $$\n",
        "\n",
        "The REINFORCE algorithm approximates this quantity from N trajectories as follows:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\Biggl( \\sum_{t=0}^{T-1} \\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it})\\Biggl) \\Biggl(\\sum_{t=0}^{T-1} r(s_{it}, a_{it}) \\Biggl) $$\n",
        "\n",
        "**Note:** You will implement a slightly modified version of REINFORCE that uses the total discounted reward (with discount factor $\\gamma$) rather than the total reward in order to encourage the agent to prioritize more immediate rewards over those in the distant future:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T-1} \\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it}) R_i $$\n",
        "\n",
        "where $R_i$ is the total discounted reward for trajectory $i$:\n",
        "\n",
        "$$ R_i = \\sum_{t=0}^{T-1} \\gamma^{t} r(s_{it}, a_{it}) $$\n",
        "\n",
        "\n",
        "As the goal of REINFORCE is to maximize $J(\\theta)$, our loss function $L(\\theta)$ (which we will be minimizing) will be the following:\n",
        "\n",
        "$$ L(\\theta) = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T-1} log \\pi_\\theta(a_{it}|s_{it}) R_i$$\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "You will need to implement the following:\n",
        "\n",
        "1. `PolicyNet` class - This class will define the policy network used in the REINFORCE algorithm.\n",
        "\n",
        "The policy will be a MLP network that outputs a **Normal** distribution for a given state:\n",
        "\n",
        "$$\\pi(a_t | s_t) \\sim N(\\mu(s_t), \\sigma(s_t)) $$\n",
        "where\n",
        "\n",
        "$$\\mu(s_t), \\sigma(s_t)$$\n",
        "\n",
        "are the outputs of the `PolicyNet`.\n",
        "\n",
        "2. `PolicyGradient` class - This class will define the REINFORCE algorithm.\n",
        "\n",
        "\n",
        "Follow the instructions below to implement each of these components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0h48TdcTsyl"
      },
      "source": [
        "### `PolicyNet` class\n",
        "\n",
        "The `PolicyNet` class should define a neural network that takes in a state and outputs a probability distribution over the action space. The network should have the following architecture:\n",
        "\n",
        "- Input layer: a fully-connected layer with `state_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Output layer: a fully-connected layer with `hidden_dim` input nodes and `action_dim * 2` output nodes (first half for $\\mu(s_t)$, second half for $\\log(\\sigma(s_t))$).\n",
        "\n",
        "\n",
        "Finally, after the output layer, return a `torch.distributions.normal.Normal` object with the proper $\\mu(s_t)$ and $\\sigma(s_t)$.\n",
        "\n",
        "[Documentation on `torch.distributions.normal.Normal`](https://pytorch.org/docs/stable/distributions.html#normal)\n",
        "\n",
        "> You should use the `nn` module of PyTorch to define this network, and use `D.Normal` to construct the final return distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErpoIXHZU_nK"
      },
      "outputs": [],
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int):\n",
        "        \"\"\"Policy network for the REINFORCE algorithm.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): Dimension of the state space.\n",
        "            action_dim (int): Dimension of the action space.\n",
        "            hidden_dim (int): Dimension of the hidden layers.\n",
        "        \"\"\"\n",
        "        super(PolicyNet, self).__init__()\n",
        "        # TODO: Implement the policy network for the REINFORCE algorithm here\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> D.Normal:\n",
        "        \"\"\"Forward pass of the policy network.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): State of the environment. Shape (N, state_dim)\n",
        "\n",
        "\n",
        "        Returns:\n",
        "            action_dist (D.Normal): Normal distribution representing \\pi(a_t | s_t)\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass of the policy network here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAZMJx-eqLpm"
      },
      "source": [
        "\n",
        "### `PolicyGradient` class\n",
        "\n",
        "The `PolicyGradient` class should define the REINFORCE algorithm.\n",
        "The `PolicyGradient` class should have the following methods:\n",
        "\n",
        "- `__init__(self, env, policy_net, seed, reward_to_go: bool = False)`: Constructor method that initializes the environment and policy network.\n",
        "\n",
        "- `compute_action(self, state)`: Method that computes an action based on the policy network.\n",
        "\n",
        "- `compute_loss(self, episode, gamma)`: Method that computes the loss for a given episode.\n",
        "\n",
        "- `update_policy(self, episodes, optimizer, gamma)`: Method that updates the policy network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHA2HN-AqM-N"
      },
      "outputs": [],
      "source": [
        "class PolicyGradient:\n",
        "    def __init__(\n",
        "      self,\n",
        "      policy_net : PolicyNet,\n",
        "      reward_to_go: bool = False\n",
        "    ):\n",
        "      \"\"\"Policy gradient algorithm based on the REINFORCE algorithm.\n",
        "\n",
        "      Args:\n",
        "          policy_net (PolicyNet): Policy network\n",
        "          reward_to_go (bool): True if using reward_to_go, False if not (False in part 1, True in part 2)\n",
        "      \"\"\"\n",
        "      self.policy_net = policy_net\n",
        "      self.reward_to_go = reward_to_go\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "      return next(iter(self.policy_net.parameters())).device\n",
        "\n",
        "    def to(self, device : str | torch.device):\n",
        "      self.policy_net.to(device)\n",
        "      return self\n",
        "\n",
        "    def compute_action(self, state : np.ndarray) -> np.ndarray:\n",
        "      \"\"\"Select an action based on the policy network\n",
        "\n",
        "      Args:\n",
        "          state (np.ndarray): State of the environment\n",
        "\n",
        "      Returns:\n",
        "          action (np.ndarray): Action to take\n",
        "      \"\"\"\n",
        "      with torch.no_grad():\n",
        "        # TODO: Implement the action selection here based on the policy network output probabilities\n",
        "        # You also need to convert between numpy arrays and torch tensors\n",
        "\n",
        "\n",
        "    def compute_loss(\n",
        "      self,\n",
        "      episode : list[tuple[\n",
        "          np.ndarray, np.ndarray, float\n",
        "      ]],\n",
        "      gamma : float\n",
        "    ) -> torch.Tensor:\n",
        "      \"\"\"Compute the loss function J for the REINFORCE algorithm\n",
        "\n",
        "      Args:\n",
        "          episode (list): List of tuples (state, action, reward)\n",
        "          gamma (float): Discount factor\n",
        "\n",
        "      Returns:\n",
        "          loss (torch.Tensor): The value of the loss function\n",
        "      \"\"\"\n",
        "      # TODO: Extract states, actions and rewards from the episode, and maybe convert them to torch\n",
        "      if not self.reward_to_go:\n",
        "        # TODO: Part 1: Remove the following line that starts with \"raise\", and compute the total discounted reward here\n",
        "        raise NotImplementedError(\"Please implement part 1\")\n",
        "\n",
        "      else:\n",
        "        # TODO: Part 2: Remove the following line that starts with \"raise\", and compute the discounted rewards to go here\n",
        "        raise NotImplementedError(\"Please implement part 2\")\n",
        "\n",
        "      # TODO: Implement the loss function for the REINFORCE algorithm here\n",
        "\n",
        "      return loss\n",
        "\n",
        "    def update_policy(self, episodes, optimizer, gamma):\n",
        "      \"\"\"Update the policy network using the batch of episodes\n",
        "\n",
        "      Args:\n",
        "          episodes (list): List of episodes\n",
        "          optimizer (torch.optim): Optimizer\n",
        "          gamma (float): Discount factor\n",
        "      Returns:\n",
        "          loss (float): The value of the loss function\n",
        "      \"\"\"\n",
        "      # TODO: Compute the loss function for each episode using compute_loss\n",
        "\n",
        "\n",
        "      # TODO: Update the policy network using average loss across the batch\n",
        "\n",
        "\n",
        "      # TODO: Return the loss value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4J0zk58fHE1"
      },
      "outputs": [],
      "source": [
        "def collect_episode(env : gym.Env, policy : PolicyGradient | None = None, seed : int | None = None):\n",
        "  \"\"\"\n",
        "  Run an episode of the environment and return the episode\n",
        "\n",
        "  Returns:\n",
        "      episode (list): List of tuples (state, action, reward)\n",
        "  \"\"\"\n",
        "  state, info = env.reset(seed=seed)\n",
        "  episode = []\n",
        "  done = False\n",
        "  while not done:\n",
        "    if policy is not None:\n",
        "      action = policy.compute_action(state)\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    episode.append((state, action, reward))\n",
        "    state = next_state\n",
        "  return episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b8PncKufaOD"
      },
      "outputs": [],
      "source": [
        "def evaluate(env : gym.Env, policy : PolicyGradient | None = None, num_episodes = 100):\n",
        "  \"\"\"Evaluate the policy network by running multiple episodes.\n",
        "\n",
        "  Args:\n",
        "      env (gym.Env): Environment to evaluate the policy network on\n",
        "      policy (PolicyGradient): Policy network to evaluate\n",
        "      num_episodes (int): Number of episodes to run\n",
        "\n",
        "  Returns:\n",
        "      average_reward (float): Average total reward per episode\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for i in range(num_episodes):\n",
        "    episode = collect_episode(env, policy)\n",
        "    episode_rewards.append(sum([reward for _, _, reward in episode]))\n",
        "  return np.mean(episode_rewards)\n",
        "\n",
        "def train(env : gym.Env, policy : PolicyGradient, num_iterations : int, batch_size : int, gamma : float, lr : float) -> None:\n",
        "  \"\"\"Train the policy network using the REINFORCE algorithm\n",
        "\n",
        "  Args:\n",
        "      num_iterations (int): Number of iterations to train the policy network\n",
        "      batch_size (int): Number of episodes per batch\n",
        "      gamma (float): Discount factor\n",
        "      lr (float): Learning rate\n",
        "  \"\"\"\n",
        "  policy.policy_net.train()\n",
        "  optimizer = optim.Adam(policy.policy_net.parameters(), lr=lr)\n",
        "  losses = []\n",
        "  # Update the policy every iteration, and use one batch per iteration.\n",
        "  # Also append computed losses (return values of `update_policy`) to `losses`\n",
        "  progress = tqdm.trange(num_iterations)\n",
        "  for iter_i in progress:\n",
        "    # TODO: Implement the training loop for the REINFORCE algorithm here and add the loss of this iteration to the `losses` array\n",
        "\n",
        "    losses.append(loss)\n",
        "    progress.set_postfix(loss=loss)\n",
        "\n",
        "  return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVULWyG8R1KO"
      },
      "source": [
        "Now that you have implemented REINFORCE, it is time to train and evaluate a policy. Below, we provide training hyperparameters which you should use in your experiments. See the writeup for additional instructions on what metrics and figures you need to include in your submission.\n",
        "\n",
        "* Expected Training Time (Colab CPU): 15 minutes\n",
        "* Expected Episodic Reward: 0.5-0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzgaeNoxRXvK",
        "outputId": "ca6ff903-81de-4912-9e2b-e505c93f73a2"
      },
      "outputs": [],
      "source": [
        "print(\"Pre-training average return\", evaluate(env))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbY-aBaXRaqf",
        "outputId": "80cd4a5f-e90a-4d6a-97ad-ab4c1f810999"
      },
      "outputs": [],
      "source": [
        "reseed(seed)\n",
        "env.reset(seed=seed)\n",
        "env.action_space.seed(seed)\n",
        "env.observation_space.seed(seed)\n",
        "\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Action Space\", env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKTnD7CNxSAX",
        "outputId": "43060ce7-dc59-46f3-c219-90930471ed5b"
      },
      "outputs": [],
      "source": [
        "policy_net = PolicyNet(env.observation_space.shape[-1], env.action_space.shape[-1], 128)\n",
        "reinforce = PolicyGradient(policy_net, reward_to_go=False)\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "  device = \"mps\"\n",
        "reinforce.to(device)\n",
        "\n",
        "print(f\"Training on device {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "bkZPQpezxUkG",
        "outputId": "8b3f7181-43ec-49b6-a424-bf28273491d6"
      },
      "outputs": [],
      "source": [
        "losses = train(env, reinforce, num_iterations=300, batch_size=16, gamma=0.99, lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ1m8055eb6d",
        "outputId": "df744c18-67c9-43c8-c9ed-4a81af25a692"
      },
      "outputs": [],
      "source": [
        "evaluate(env, reinforce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdAbfPvGEhZm"
      },
      "source": [
        "Let's now plot the losses we get during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "AUzZU-G2Gxza",
        "outputId": "0bfc0561-7da3-40e8-8856-2c83069f4c63"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONMV-8qPEt75"
      },
      "source": [
        "Why did the loss go up?\n",
        "\n",
        "- At the start of training, by taking random actions the policy mostly performs random actions and receives mostly negative rewards, when $R_i$ is negative, our loss will be negative since `log_prob` will always be `<= 0`.\n",
        "  - Ask yourself: what kind of gradient is being propagated when the losses are all negative?\n",
        "  - The answer is that we're pushing down (minimizing log-likelihood) on all actions we've been executing so far. However, we're minimizing log-likelihood more on actions that produce smaller $R_i$.\n",
        "- As we optimize this negative loss, we will start to see some positive rewards, and with a positive $R_i$ our loss will be positive.\n",
        "  - Now think about what kind of gradient is being propagated when the losses are all positive?\n",
        "  - We're pushing up (maximizing log-likelihood) on all actions that producing positive $R_i$s. However, we're pushing up harder on actions that produce bigger $R_i$.\n",
        "\n",
        "A very natural thing to think is that can we add a baseline function $b(s_t)$ (ideally, the **mean** value of $R_i$ of following the policy distribution) so that\n",
        "\n",
        "- For actions better than the mean value $b(s_t)$, we can **push up** the density function of the action distribution\n",
        "- For actions worse than the mean value $b(s_t)$, we can **push down** the density function of the action distribution\n",
        "\n",
        "We're effectively reducing the variance on the gradient of the policy. This will be further discussed in the next assignment when you work on actor-critic algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzzGu2i-cftP"
      },
      "source": [
        "Now let's save the policy and visualize it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "seqsRDRsiVjT",
        "outputId": "127a2981-4766-4a04-9436-e6ad4c07fd88"
      },
      "outputs": [],
      "source": [
        "# Let's save and visualize our policy\n",
        "torch.save(reinforce.policy_net.state_dict(), \"reinforce.pt\")\n",
        "visualize(env, algorithm=reinforce, video_name=\"reinforce\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtRRbGf_0R5A"
      },
      "source": [
        "# Part 2: REINFORCE with Reward-to-Go\n",
        "\n",
        "In this part of the assignment, we will modify the REINFORCE algorithm to use the (discounted) reward-to-go instead of the total discounted reward.\n",
        "\n",
        "To decrease the variance of the policy gradient, one approach is to make use of causality by observing that the policy cannot influence past rewards. This results in a revised objective where the total rewards only include those acquired after the policy is evaluated. These rewards are considered a sample estimation of the Q function and are known as the \"reward-to-go\".\n",
        "\n",
        "## Overview\n",
        "\n",
        "The reward-to-go is defined as:\n",
        "\n",
        "$$ R_t = \\sum_{t'=t}^{T-1} \\gamma^{t'-t} r(s_{t'}, a_{t'}) $$\n",
        "\n",
        "\n",
        "The updated policy gradient will look like this (same as before, except discounted reward-to-go instead of discounted total reward):\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N}  \\sum_{t=0}^{T-1} \\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it}) R_{it} $$\n",
        "\n",
        "Thus, the updated loss function will be:\n",
        "\n",
        "$$ L(\\theta) = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T-1} log \\pi_\\theta(a_{it}|s_{it}) R_{it}$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Modify the `compute_loss` method to allow for the use of reward-to-go instead of the total discounted reward.\n",
        "\n",
        "Now that you have implemented REINFORCE with reward-to-go, it is time to train and evaluate a policy. Below, we provide training hyperparameters which you should use in your experiments. See the writeup for additional instructions on what metrics and figures you need to include in your submission.\n",
        "\n",
        "* Expected Training Time (Colab CPU): 15 minutes\n",
        "* Expected Reward (when calling `evaluate(100)`): 0.9-1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QlGoFBW07pX",
        "outputId": "08e74ff3-6828-45ec-a328-ee7e67f72108"
      },
      "outputs": [],
      "source": [
        "reseed(seed)\n",
        "env.reset(seed=seed)\n",
        "env.action_space.seed(seed)\n",
        "env.observation_space.seed(seed)\n",
        "\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Action Space\", env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0JffEYedbrA",
        "outputId": "f728d7f9-5041-43d9-9ff4-e18d183f402d"
      },
      "outputs": [],
      "source": [
        "policy_net = PolicyNet(env.observation_space.shape[-1], env.action_space.shape[-1], 128)\n",
        "reinforce = PolicyGradient(policy_net, reward_to_go=True)\n",
        "reinforce.to(device)\n",
        "\n",
        "reinforce.policy_net.load_state_dict(torch.load(\"reinforce.pt\", map_location=device, weights_only=True))\n",
        "print(f\"Training on device {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skRbfYpbderl",
        "outputId": "1130e306-1ff8-4c2c-ee82-2f77dbe136ae"
      },
      "outputs": [],
      "source": [
        "losses = train(env, reinforce, num_iterations=300, batch_size=16, gamma=0.99, lr=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "jhQVwwJA03OB",
        "outputId": "f0683530-0e5b-410e-ebf7-9dd64a3acb39"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86VR-A2N1-ef",
        "outputId": "93bfde55-747a-4203-f2fa-bcdf0555e97c"
      },
      "outputs": [],
      "source": [
        "evaluate(env, reinforce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "yLWG-8qC2cjb",
        "outputId": "e5ecac02-c6af-431c-9538-7739723a9cc9"
      },
      "outputs": [],
      "source": [
        "visualize(env, reinforce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de9BSF3IwK7f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d52C6fHT9eR1"
      ],
      "provenance": []
    },
    "interpreter": {
      "hash": "f80e6f76365c3a1e577e14ae81718f2e1aeab33a4811983ca756090a3b3aa652"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
